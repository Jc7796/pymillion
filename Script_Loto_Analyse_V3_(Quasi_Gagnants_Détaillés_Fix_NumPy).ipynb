{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jc7796/pymillion/blob/main/Script_Loto_Analyse_V3_(Quasi_Gagnants_D%C3%A9taill%C3%A9s_Fix_NumPy).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "# from bs4 import BeautifulSoup # Suppression de l'import de BeautifulSoup\n",
        "from scipy.stats import chisquare, chi2_contingency\n",
        "from collections import Counter, defaultdict # defaultdict ajouté\n",
        "from itertools import combinations, product\n",
        "import time\n",
        "from math import comb as nCr\n",
        "import json\n",
        "import random\n",
        "\n",
        "# --- Machine Learning Imports ---\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import statsmodels.api as sm # Pour l'analyse de saisonnalité/cycles (ACF/PACF)\n",
        "from statsmodels.tsa.arima.model import ARIMA # Pour les modèles AR, MA, ARMA, ARIMA\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox # Pour le test de Ljung-Box sur les résidus\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from prefixspan import PrefixSpan\n",
        "import pmdarima as pm # Ajout de l'import pour auto_arima\n",
        "\n",
        "# Configuration logs\n",
        "logging.basicConfig(level=logging.INFO, filename='loto_statistical_analysis.log', filemode='a',\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Dictionnaire pour la conversion des mois français (mis à jour)\n",
        "MOIS_MAP = {\n",
        "    'janvier': '01', 'février': '02', 'fevrier': '02', 'mars': '03',\n",
        "    'avril': '04', 'mai': '05', 'juin': '06', 'juillet': '07',\n",
        "    'août': '08', 'aout': '08', 'septembre': '09', 'octobre': '10',\n",
        "    'novembre': '11', 'décembre': '12', 'decembre': '12'\n",
        "}\n",
        "\n",
        "# --- Paramètres Globaux ---\n",
        "USE_GRID_SEARCH_FOR_FULL_ANALYSIS_ML = True\n",
        "MAX_GRID_SEARCH_CUSTOM_COMBINATIONS = 133\n",
        "RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS = True\n",
        "RUN_BACKTESTING_ML_IN_FULL_ANALYSIS = True\n",
        "RUN_DEEP_DIVE_COMPLEMENTARY_9 = True\n",
        "RUN_DEEP_DIVE_CLUSTER_5 = True\n",
        "RUN_TRANSITION_PROBABILITIES = True\n",
        "RUN_SLIDING_WINDOW_ENHANCED_METRICS = True\n",
        "RUN_SLIDING_WINDOW_TIMESERIES_ANALYSIS = True\n",
        "RUN_DRAW_CLUSTERING = True\n",
        "RUN_ALMOST_WINNERS_ANALYSIS_IN_BACKTEST = True # S'assurer que c'est True\n",
        "RUN_PREFIXSPAN_ANALYSIS = True\n",
        "\n",
        "# --- Drapeau pour choisir le mode d'exécution ---\n",
        "RUN_LAMBDA_STRATEGY_MODE = False\n",
        "\n",
        "# --- Vos Numéros Joués ---\n",
        "MES_NUMEROS_PRINCIPAUX_JOUES = {6, 7, 9, 30}\n",
        "MES_COMPLEMENTAIRES_POSSIBLES_JOUES = {6, 7, 9}\n",
        "\n",
        "# --- Configuration Loto pour le mode lambda ---\n",
        "CONFIG_LOTO_LAMBDA = {\n",
        "    \"total_numbers\": 49, # Max number for main draw\n",
        "    \"numbers_to_choose\": 5,\n",
        "    \"total_lucky_numbers\": 10, # Max number for complementary\n",
        "    \"lucky_numbers_to_choose\": 1,\n",
        "    \"min_val_number\": 1,\n",
        "    \"min_val_lucky\": 1\n",
        "}\n",
        "\n",
        "# --- Fonctions de base ---\n",
        "def convert_french_date(date_text):\n",
        "    if not isinstance(date_text, str):\n",
        "        return pd.NaT\n",
        "    date_text_cleaned = date_text.strip()\n",
        "    parts = date_text_cleaned.lower().split()\n",
        "    if len(parts) == 3:\n",
        "        day, month_fr, year = parts\n",
        "        month_num = MOIS_MAP.get(month_fr)\n",
        "        if month_num and year.isdigit() and day.isdigit():\n",
        "            try:\n",
        "                day_int = int(day)\n",
        "                if 1 <= day_int <= 31:\n",
        "                    formatted_date_str = f\"{day.zfill(2)}-{month_num}-{year}\"\n",
        "                    dt_obj = pd.to_datetime(formatted_date_str, format=\"%d-%m-%Y\", errors='coerce')\n",
        "                    if not pd.NaT is dt_obj: return dt_obj\n",
        "            except ValueError:\n",
        "                logging.debug(f\"Erreur de conversion du jour en entier pour '{date_text_cleaned}'.\")\n",
        "    dt_obj_fallback = pd.to_datetime(date_text_cleaned, dayfirst=True, errors='coerce')\n",
        "    if pd.NaT is dt_obj_fallback:\n",
        "        logging.warning(f\"Impossible de convertir la date: '{date_text_cleaned}' (toutes les tentatives ont échoué).\")\n",
        "    return dt_obj_fallback\n",
        "\n",
        "def fetch_lotto_data(page_url=\"http://loto.akroweb.fr/loto-historique-tirages/\"):\n",
        "    all_lotto_data_dfs = []\n",
        "    current_url, pages_fetched_count = page_url, 0\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36'}\n",
        "    expected_page_columns = ['Jour', 'Date', 'N1', 'N2', 'N3', 'N4', 'N5', 'Complementaire']\n",
        "    while current_url and pages_fetched_count < 1:\n",
        "        logging.info(f\"Récupération des données depuis : {current_url}\")\n",
        "        print(f\"Récupération des données depuis : {current_url}\")\n",
        "        pages_fetched_count += 1\n",
        "        lotto_data_page = pd.DataFrame(columns=expected_page_columns)\n",
        "        try:\n",
        "            response = requests.get(current_url, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            try:\n",
        "                raw_dfs_list = pd.read_html(StringIO(response.text), header=None, keep_default_na=False, na_values=[''])\n",
        "                if raw_dfs_list:\n",
        "                    raw_df = raw_dfs_list[0]\n",
        "                    temp_data = []\n",
        "                    if raw_df.shape[1] >= 10:\n",
        "                        for idx, row in raw_df.iterrows():\n",
        "                            try:\n",
        "                                if len(str(row.iloc[2]).strip()) > 5 and str(row.iloc[4]).strip().isdigit():\n",
        "                                    jour, date_str = str(row.iloc[1]).strip(), str(row.iloc[2]).strip()\n",
        "                                    nums_str = [str(row.iloc[i]).strip() for i in range(4, 10)]\n",
        "                                    if all(s.isdigit() for s in nums_str) and len(nums_str) == 6:\n",
        "                                        temp_data.append([jour, date_str] + nums_str)\n",
        "                            except IndexError: logging.warning(f\"Ligne {idx} raw_df (interne): IndexError. {row.tolist()}\")\n",
        "                            except Exception as e_row_proc: logging.error(f\"Erreur ligne {idx} raw_df (interne): {e_row_proc}. {row.tolist()}\")\n",
        "                        if temp_data: lotto_data_page = pd.DataFrame(temp_data, columns=expected_page_columns)\n",
        "                        else: logging.warning(\"Aucune donnée exploitable formatée (raw_df).\")\n",
        "                    else: logging.warning(f\"raw_df < 10 colonnes ({raw_df.shape[1]}).\")\n",
        "                else: logging.warning(f\"pd.read_html (header=None) n'a retourné aucune table sur {current_url}.\")\n",
        "            except ValueError as ve: logging.warning(f\"pd.read_html n'a trouvé aucune table sur {current_url}: {ve}\")\n",
        "            except Exception as e_pd_read: logging.error(f\"Erreur pd.read_html sur {current_url}: {e_pd_read}\")\n",
        "\n",
        "            if not lotto_data_page.empty:\n",
        "                num_cols = [\"N1\", \"N2\", \"N3\", \"N4\", \"N5\", \"Complementaire\"]\n",
        "                for col in num_cols:\n",
        "                    if col in lotto_data_page.columns:\n",
        "                        lotto_data_page[col] = pd.to_numeric(lotto_data_page[col], errors=\"coerce\")\n",
        "                        if not lotto_data_page[col].isnull().all(): lotto_data_page[col] = lotto_data_page[col].astype('Int64')\n",
        "\n",
        "                if 'Date' in lotto_data_page.columns:\n",
        "                    lotto_data_page['Date_avant_conversion'] = lotto_data_page['Date']\n",
        "                    lotto_data_page['Date'] = lotto_data_page['Date'].apply(convert_french_date)\n",
        "                    lotto_data_page.dropna(subset=['Date'], inplace=True)\n",
        "\n",
        "                if num_cols_exist := [c for c in num_cols if c in lotto_data_page.columns]:\n",
        "                    lotto_data_page.dropna(subset=num_cols_exist, how='any', inplace=True)\n",
        "\n",
        "                if not lotto_data_page.empty: all_lotto_data_dfs.append(lotto_data_page)\n",
        "                else: logging.warning(f\"Aucune ligne valide après nettoyage pour {current_url}.\")\n",
        "            else: logging.warning(f\"lotto_data_page vide pour {current_url} avant nettoyage final.\")\n",
        "            current_url = None\n",
        "        except requests.exceptions.RequestException as e: logging.error(f\"Erreur réseau {current_url}: {e}\"); current_url = None\n",
        "        except Exception as e: logging.error(f\"Erreur boucle {current_url}: {e}\", exc_info=True); current_url = None\n",
        "\n",
        "    if not all_lotto_data_dfs: logging.warning(\"Aucune donnée collectée.\"); return pd.DataFrame(columns=expected_page_columns)\n",
        "\n",
        "    final_df = pd.concat(all_lotto_data_dfs, ignore_index=True)\n",
        "    final_num_cols = [\"N1\", \"N2\", \"N3\", \"N4\", \"N5\", \"Complementaire\"]\n",
        "    for col in final_num_cols:\n",
        "        if col in final_df.columns:\n",
        "            final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "            if not final_df[col].isnull().all(): final_df[col] = final_df[col].astype('Int64')\n",
        "\n",
        "    if 'Date' in final_df.columns:\n",
        "        final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')\n",
        "        final_df.dropna(subset=['Date'], inplace=True)\n",
        "        final_df.sort_values(by='Date', ascending=False, inplace=True); final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    if main_num_cols := [c for c in [\"N1\",\"N2\",\"N3\",\"N4\",\"N5\"] if c in final_df.columns]:\n",
        "        final_df.dropna(subset=main_num_cols, how='all', inplace=True)\n",
        "\n",
        "    print(f\"Données finales après fetch: {final_df.shape}, Colonnes: {final_df.columns.tolist()}\")\n",
        "    if not final_df.empty: print(final_df.head(3).to_string())\n",
        "    else: print(\"DataFrame final vide après fetch.\")\n",
        "    return final_df\n",
        "\n",
        "# --- Fonctions pour la Stratégie Lambda ---\n",
        "def calculer_probabilites_lambda(config):\n",
        "    total_comb_principaux = nCr(config[\"total_numbers\"], config[\"numbers_to_choose\"])\n",
        "    total_comb_chance = nCr(config[\"total_lucky_numbers\"], config[\"lucky_numbers_to_choose\"])\n",
        "    proba_rang1 = 1 / (total_comb_principaux * total_comb_chance)\n",
        "    proba_5_bons_sans_chance = (1 / total_comb_principaux) * \\\n",
        "                               (nCr(config[\"lucky_numbers_to_choose\"], 0) * nCr(config[\"total_lucky_numbers\"] - config[\"lucky_numbers_to_choose\"], config[\"lucky_numbers_to_choose\"]) / total_comb_chance)\n",
        "\n",
        "    print(\"--- Probabilités (Exemple Loto France) ---\")\n",
        "    print(f\"Nombre total de combinaisons principales : {total_comb_principaux:,}\")\n",
        "    print(f\"Nombre total de combinaisons de N° Chance : {total_comb_chance:,}\")\n",
        "    print(f\"Nombre total de combinaisons (Rang 1 - 5N + 1C) : {total_comb_principaux * total_comb_chance:,}\")\n",
        "    print(f\"Probabilité de gagner le Rang 1 (5N + 1C) : 1 chance sur {1/proba_rang1:,.0f}\")\n",
        "    if proba_5_bons_sans_chance > 0 :\n",
        "        print(f\"Probabilité d'avoir 5 bons numéros (sans N°C) : 1 chance sur {1/proba_5_bons_sans_chance:,.0f} (approximatif)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "def analyser_donnees_historiques_simplifie_lambda(df_historique, config):\n",
        "    print(\"\\n[INFO] Lancement des analyses statistiques simplifiées sur l'historique...\")\n",
        "    if df_historique.empty:\n",
        "        print(\"[ATTENTION] Historique vide, analyses statistiques non effectuées.\")\n",
        "        return None, None\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, config[\"numbers_to_choose\"] + 1)]\n",
        "    frequences_principales_dict, frequences_chance_dict = None, None\n",
        "\n",
        "    try:\n",
        "        valid_num_cols = [col for col in num_cols if col in df_historique.columns]\n",
        "        if valid_num_cols:\n",
        "            all_numbers_s = df_historique[valid_num_cols].stack()\n",
        "            all_numbers = pd.to_numeric(all_numbers_s, errors='coerce').dropna().astype(int).values\n",
        "            if all_numbers.size > 0:\n",
        "                frequences_principales_dict = Counter(all_numbers)\n",
        "                print(\"\\n--- Fréquences des Numéros Principaux (Top 5) ---\")\n",
        "                for num, count in frequences_principales_dict.most_common(5): print(f\"Numéro {num}: {count} fois\")\n",
        "            else: print(\"[INFO] Pas de numéros principaux valides trouvés pour l'analyse de fréquence.\")\n",
        "        else: print(\"[ATTENTION] Colonnes de numéros principaux manquantes dans l'historique.\")\n",
        "\n",
        "        if 'Complementaire' in df_historique.columns:\n",
        "            lucky_numbers_s = df_historique['Complementaire']\n",
        "            lucky_numbers = pd.to_numeric(lucky_numbers_s, errors='coerce').dropna().astype(int).values\n",
        "            if lucky_numbers.size > 0:\n",
        "                frequences_chance_dict = Counter(lucky_numbers)\n",
        "                print(\"\\n--- Fréquences des N° Chance (Top 3) ---\")\n",
        "                for num, count in frequences_chance_dict.most_common(3): print(f\"N° Chance {num}: {count} fois\")\n",
        "            else: print(\"[INFO] Pas de numéros chance valides trouvés pour l'analyse de fréquence.\")\n",
        "        else: print(\"[ATTENTION] Colonne 'Complementaire' manquante dans l'historique.\")\n",
        "    except Exception as e: print(f\"[ERREUR] lors de l'analyse simplifiée des fréquences: {e}\")\n",
        "\n",
        "    print(\"[INFO] Analyses statistiques simplifiées terminées.\")\n",
        "    return frequences_principales_dict, frequences_chance_dict\n",
        "\n",
        "def generer_combinaison_aleatoire_lambda(config):\n",
        "    numeros_principaux = sorted(random.sample(range(config[\"min_val_number\"], config[\"total_numbers\"] + 1), config[\"numbers_to_choose\"]))\n",
        "    numero_chance_list = random.sample(range(config[\"min_val_lucky\"], config[\"total_lucky_numbers\"] + 1), config[\"lucky_numbers_to_choose\"])\n",
        "    return numeros_principaux, numero_chance_list[0]\n",
        "\n",
        "def choisir_numeros_anti_popularite_lambda(config, df_historique=None, top_n_sums_to_avoid=5):\n",
        "    print(\"\\n[STRATEGIE] Génération d'une combinaison 'moins populaire'...\")\n",
        "    if df_historique is None or df_historique.empty:\n",
        "        print(\"[ATTENTION] Pas de données historiques pour 'anti-popularité', génération aléatoire classique.\")\n",
        "        return generer_combinaison_aleatoire_lambda(config)\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, config[\"numbers_to_choose\"] + 1)]\n",
        "    valid_num_cols = [col for col in num_cols if col in df_historique.columns]\n",
        "    sommes_a_eviter = set()\n",
        "\n",
        "    if valid_num_cols:\n",
        "        try:\n",
        "            df_historique_nums = df_historique[valid_num_cols].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "            if not df_historique_nums.empty:\n",
        "                sommes_historiques = df_historique_nums.sum(axis=1)\n",
        "                if not sommes_historiques.empty:\n",
        "                    frequence_sommes = Counter(sommes_historiques)\n",
        "                    sommes_a_eviter = {somme for somme, count in frequence_sommes.most_common(top_n_sums_to_avoid)}\n",
        "                    print(f\"  Sommes de numéros principaux à éviter: {sommes_a_eviter if sommes_a_eviter else 'Aucune'}\")\n",
        "                else: print(\"[INFO] Pas de sommes historiques à analyser.\")\n",
        "            else: print(\"[ATTENTION] Pas de données numériques valides pour les sommes historiques.\")\n",
        "        except Exception as e: print(f\"[ERREUR] Calcul des sommes historiques échoué: {e}.\")\n",
        "\n",
        "    tentatives = 0\n",
        "    while tentatives < 100:\n",
        "        principaux, chance = generer_combinaison_aleatoire_lambda(config)\n",
        "        somme_principaux = sum(principaux)\n",
        "        nb_bas = sum(1 for n in principaux if n <= 31) # Numéros \"dates\"\n",
        "        if somme_principaux not in sommes_a_eviter and nb_bas <= (config[\"numbers_to_choose\"] // 2 + 1) : # Eviter trop de \"dates\"\n",
        "            print(f\"  Combinaison 'moins populaire' générée: {principaux} + {chance} (Somme: {somme_principaux}, N° bas: {nb_bas})\")\n",
        "            return principaux, chance\n",
        "        tentatives += 1\n",
        "    print(\"[ATTENTION] N'a pas pu générer une combinaison 'moins populaire' distincte, retour à l'aléatoire.\")\n",
        "    return generer_combinaison_aleatoire_lambda(config)\n",
        "\n",
        "def choisir_numeros_froids_lambda(config, frequences_principales=None, frequences_chance=None, nombre_a_considerer=15):\n",
        "    print(f\"\\n[STRATEGIE] Choix de numéros 'froids' (basée sur les moins fréquents)...\")\n",
        "    principaux_froids = []\n",
        "    if frequences_principales:\n",
        "        tous_les_numeros_p = list(range(config[\"min_val_number\"], config[\"total_numbers\"] + 1))\n",
        "        numeros_tries_par_frequence_p = sorted(tous_les_numeros_p, key=lambda x: frequences_principales.get(x, 0))\n",
        "        candidats_froids_p = numeros_tries_par_frequence_p[:nombre_a_considerer]\n",
        "        if len(candidats_froids_p) >= config[\"numbers_to_choose\"]:\n",
        "            principaux_froids = sorted(random.sample(candidats_froids_p, config[\"numbers_to_choose\"]))\n",
        "        else:\n",
        "            print(f\"[ATTENTION] Pas assez de numéros principaux 'froids' distincts ({len(candidats_froids_p)} trouvés), complétion aléatoire.\")\n",
        "            principaux_froids = sorted(list(set(candidats_froids_p))) # Prendre ce qu'on a\n",
        "            needed = config[\"numbers_to_choose\"] - len(principaux_froids)\n",
        "            if needed > 0:\n",
        "                pool_restant = [n for n in tous_les_numeros_p if n not in principaux_froids]\n",
        "                if len(pool_restant) >= needed:\n",
        "                    principaux_froids.extend(random.sample(pool_restant, needed))\n",
        "                else: # Si toujours pas assez, prendre tout ce qui reste\n",
        "                    principaux_froids.extend(pool_restant)\n",
        "                principaux_froids = sorted(principaux_froids)\n",
        "                if len(principaux_froids) < config[\"numbers_to_choose\"]: # Ultime vérification\n",
        "                    print(f\"[ATTENTION] Impossible de former une combinaison complète de numéros froids. Numéros actuels: {principaux_froids}\")\n",
        "    if not principaux_froids or len(principaux_froids) < config[\"numbers_to_choose\"]:\n",
        "        print(\"[ATTENTION] Impossible de déterminer les numéros principaux 'froids' ou pas assez, génération aléatoire.\")\n",
        "        principaux_froids, _ = generer_combinaison_aleatoire_lambda(config) # Retourne une combinaison complète\n",
        "\n",
        "    chance_froide = None\n",
        "    if frequences_chance:\n",
        "        tous_les_numeros_c = list(range(config[\"min_val_lucky\"], config[\"total_lucky_numbers\"] + 1))\n",
        "        numeros_tries_par_frequence_c = sorted(tous_les_numeros_c, key=lambda x: frequences_chance.get(x, 0))\n",
        "        candidats_froids_c = numeros_tries_par_frequence_c[:max(1, config[\"lucky_numbers_to_choose\"] + 2)] # Un peu plus de choix\n",
        "        if candidats_froids_c:\n",
        "            chance_froide = random.choice(candidats_froids_c)\n",
        "    if chance_froide is None:\n",
        "        print(\"[ATTENTION] Impossible de déterminer un N° Chance 'froid', génération aléatoire.\")\n",
        "        _, chance_froide = generer_combinaison_aleatoire_lambda(config)\n",
        "\n",
        "    print(f\"  Combinaison 'froide' suggérée: {principaux_froids} + {chance_froide}\")\n",
        "    return principaux_froids, chance_froide\n",
        "\n",
        "def enregistrer_jeu_lambda(date_str, numeros_principaux, numero_chance, mise=2.50):\n",
        "    print(f\"\\n[JEU ENREGISTRÉ] Date: {date_str}, Numéros: {numeros_principaux}, Chance: {numero_chance}, Mise: {mise}€\")\n",
        "\n",
        "def strategie_loto_lambda_main():\n",
        "    print(\"Bienvenue dans l'Assistant Loto pour Personne Lambda !\")\n",
        "    calculer_probabilites_lambda(CONFIG_LOTO_LAMBDA)\n",
        "\n",
        "    df_historique_lambda = fetch_lotto_data()\n",
        "    frequences_principales_lambda, frequences_chance_lambda = None, None\n",
        "    if df_historique_lambda is not None and not df_historique_lambda.empty:\n",
        "        frequences_principales_lambda, frequences_chance_lambda = analyser_donnees_historiques_simplifie_lambda(df_historique_lambda, CONFIG_LOTO_LAMBDA)\n",
        "    else:\n",
        "        print(\"[INFO] Aucune donnée historique chargée pour les analyses détaillées du mode lambda.\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"--- Choix de la Stratégie de Sélection des Numéros ---\")\n",
        "    print(\"1: Combinaison complètement aléatoire\")\n",
        "    print(\"2: Combinaison 'moins populaire' (basée sur sommes et peu de numéros 'date')\")\n",
        "    print(\"3: Combinaison de numéros 'froids' (basée sur les moins fréquents)\")\n",
        "    print(\"-\" * 40)\n",
        "    choix_strategie = input(\"Entrez votre choix de stratégie (1, 2, ou 3): \")\n",
        "\n",
        "    numeros_suggeres_p, numero_suggere_c = [], None\n",
        "\n",
        "    if choix_strategie == '1':\n",
        "        numeros_suggeres_p, numero_suggere_c = generer_combinaison_aleatoire_lambda(CONFIG_LOTO_LAMBDA)\n",
        "        print(f\"\\n  Combinaison aléatoire suggérée : {numeros_suggeres_p} + {numero_suggere_c}\")\n",
        "    elif choix_strategie == '2':\n",
        "        numeros_suggeres_p, numero_suggere_c = choisir_numeros_anti_popularite_lambda(CONFIG_LOTO_LAMBDA, df_historique_lambda)\n",
        "    elif choix_strategie == '3':\n",
        "        numeros_suggeres_p, numero_suggere_c = choisir_numeros_froids_lambda(CONFIG_LOTO_LAMBDA, frequences_principales_lambda, frequences_chance_lambda)\n",
        "    else:\n",
        "        print(\"\\nChoix invalide. Génération d'une combinaison aléatoire par défaut.\")\n",
        "        numeros_suggeres_p, numero_suggere_c = generer_combinaison_aleatoire_lambda(CONFIG_LOTO_LAMBDA)\n",
        "        print(f\"  Combinaison aléatoire suggérée : {numeros_suggeres_p} + {numero_suggere_c}\")\n",
        "\n",
        "    if numeros_suggeres_p and numero_suggere_c is not None:\n",
        "        date_du_jour_str = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
        "        enregistrer_jeu_lambda(date_du_jour_str, numeros_suggeres_p, numero_suggere_c)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"N'oubliez pas de jouer de manière responsable et de respecter votre budget !\")\n",
        "    print(\"Cet algorithme est un outil d'exploration et de divertissement, il ne garantit aucun gain.\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "\n",
        "# --- NOUVELLE Fonction : Modélisation des métriques de séries temporelles ---\n",
        "def analyze_timeseries_metric_models(df_results_time):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\" MODÉLISATION DES MÉTRIQUES DE FENÊTRES GLISSANTES (ARIMA)\")\n",
        "    print(f\" Hypothèse: les structures ACF/PACF ne sont pas (uniquement) des artefacts du fenêtrage.\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    metrics_to_model = {\n",
        "        'compagnons_specifiques_count': (1, 0, 0),\n",
        "        'ecart_moyen_global_fenetre': (0, 0, 1),\n",
        "        'somme_moyenne_fenetre': (1, 0, 0),\n",
        "        'variance_numeros_fenetre': (1, 0, 0)\n",
        "    }\n",
        "\n",
        "    for metric_col, order in metrics_to_model.items():\n",
        "        if metric_col in df_results_time.columns and df_results_time[metric_col].notna().sum() > 20:\n",
        "            print(f\"\\n--- Modélisation ARIMA pour '{metric_col}' ---\")\n",
        "            series = df_results_time[metric_col].dropna()\n",
        "\n",
        "            if len(series) <= 20:\n",
        "                print(f\"Pas assez de points de données ({len(series)}) pour modéliser '{metric_col}' après suppression des NaN.\")\n",
        "                continue\n",
        "\n",
        "            model_fit_final = None\n",
        "            final_order_for_plot = order\n",
        "\n",
        "            if metric_col == 'compagnons_specifiques_count':\n",
        "                print(f\"--- Recherche auto ARIMA pour '{metric_col}' ---\")\n",
        "                try:\n",
        "                    auto_model = pm.auto_arima(\n",
        "                        series,\n",
        "                        start_p=0, start_q=0,\n",
        "                        max_p=3, max_q=3,\n",
        "                        d=None,\n",
        "                        seasonal=False,\n",
        "                        stepwise=True,\n",
        "                        suppress_warnings=True,\n",
        "                        error_action='ignore',\n",
        "                        trace=False\n",
        "                    )\n",
        "                    print(f\"Meilleur ordre ARIMA trouvé par auto_arima pour '{metric_col}': {auto_model.order}\")\n",
        "                    model_fit_final = auto_model\n",
        "                    final_order_for_plot = auto_model.order\n",
        "                except Exception as e_auto_arima:\n",
        "                    print(f\"  Erreur lors de auto_arima pour '{metric_col}': {e_auto_arima}. Utilisation de l'ordre manuel {order}.\")\n",
        "\n",
        "            if model_fit_final is None:\n",
        "                print(f\"--- Utilisation de l'ordre manuel ARIMA {order} pour '{metric_col}' ---\")\n",
        "                try:\n",
        "                    model = ARIMA(series, order=order)\n",
        "                    model_fit_final = model.fit()\n",
        "                except Exception as e_manual_arima:\n",
        "                    print(f\"  Erreur lors de l'ajustement du modèle ARIMA manuel {order} pour '{metric_col}': {e_manual_arima}\")\n",
        "                    continue\n",
        "\n",
        "            # Extraction des résidus et vérification\n",
        "            residuals = getattr(model_fit_final, 'resid', None)\n",
        "            if residuals is None:\n",
        "                print(f\"  Impossible d'obtenir les résidus pour '{metric_col}'. Modèle non compatible ?\")\n",
        "                continue\n",
        "\n",
        "            if callable(residuals):\n",
        "                try:\n",
        "                    residuals = residuals()\n",
        "                except Exception as e:\n",
        "                    print(f\"  Les résidus pour '{metric_col}' sont une méthode non appelable ou échouent à l'appel : {e}\")\n",
        "                    continue\n",
        "\n",
        "            if not isinstance(residuals, (np.ndarray, pd.Series, list)):\n",
        "                print(f\"  Les résidus pour '{metric_col}' ne sont pas un tableau de nombres : {type(residuals)}\")\n",
        "                continue\n",
        "            if len(residuals) == 0:\n",
        "                print(f\"  Pas de résidus à tracer pour {metric_col}\")\n",
        "                continue\n",
        "\n",
        "            os.makedirs(\"plots\", exist_ok=True)\n",
        "            fig_res, axes_res = plt.subplots(3, 1, figsize=(12, 10))\n",
        "            axes_res[0].plot(residuals)\n",
        "            axes_res[0].set_title(f'Résidus du modèle ARIMA{final_order_for_plot} pour {metric_col}')\n",
        "            axes_res[0].grid(True)\n",
        "\n",
        "            lags_to_plot_acf_pacf = 0\n",
        "            if len(residuals) > 3:\n",
        "                lags_to_plot_acf_pacf = min(20, len(residuals) // 2 - 1)\n",
        "            if lags_to_plot_acf_pacf > 0:\n",
        "                sm.graphics.tsa.plot_acf(residuals, lags=lags_to_plot_acf_pacf, ax=axes_res[1])\n",
        "                axes_res[1].set_title(f'ACF des Résidus pour {metric_col}')\n",
        "                axes_res[1].grid(True)\n",
        "                sm.graphics.tsa.plot_pacf(residuals, lags=lags_to_plot_acf_pacf, ax=axes_res[2], method='ywm')\n",
        "                axes_res[2].set_title(f'PACF des Résidus pour {metric_col}')\n",
        "                axes_res[2].grid(True)\n",
        "            else:\n",
        "                axes_res[1].set_title(f'ACF des Résidus pour {metric_col} (non tracé - pas assez de données)')\n",
        "                axes_res[2].set_title(f'PACF des Résidus pour {metric_col} (non tracé - pas assez de données)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            order_str_filename = '_'.join(map(str, final_order_for_plot))\n",
        "            plt.savefig(f\"plots/arima_residuals_{metric_col}_order_{order_str_filename}.png\")\n",
        "            plt.close()\n",
        "            print(f\"  Graphique des résidus et ACF/PACF des résidus pour '{metric_col}' (ordre {final_order_for_plot}) sauvegardé.\")\n",
        "\n",
        "            try:\n",
        "                from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "                lags_ljungbox = None\n",
        "                if len(residuals) > 1:\n",
        "                    lags_ljungbox_val = min(10, len(residuals) - 1)\n",
        "                    if lags_ljungbox_val > 0:\n",
        "                        lags_ljungbox = [lags_ljungbox_val]\n",
        "                if lags_ljungbox:\n",
        "                    ljung_box_result = acorr_ljungbox(residuals, lags=lags_ljungbox, return_df=True)\n",
        "                    print(f\"\\n  Test de Ljung-Box sur les résidus pour '{metric_col}':\")\n",
        "                    print(ljung_box_result)\n",
        "                    if not ljung_box_result.empty and ljung_box_result['lb_pvalue'].iloc[0] > 0.05:\n",
        "                        print(f\"  Conclusion Ljung-Box: Les résidus pour '{metric_col}' semblent être indépendants (p-valeur > 0.05). Le modèle capture bien la structure.\")\n",
        "                    elif not ljung_box_result.empty:\n",
        "                        print(f\"  Conclusion Ljung-Box: Il reste de l'autocorrélation dans les résidus pour '{metric_col}' (p-valeur <= 0.05). Le modèle pourrait être amélioré.\")\n",
        "                else:\n",
        "                    print(f\"  Pas assez de résidus ({len(residuals)}) pour le test de Ljung-Box pour {metric_col}.\")\n",
        "            except Exception as e_lb:\n",
        "                print(f\"  Erreur lors du test de Ljung-Box pour '{metric_col}': {e_lb}\")\n",
        "        else:\n",
        "            print(f\"\\nPas assez de données ou métrique '{metric_col}' non trouvée pour la modélisation.\")\n",
        "\n",
        "\n",
        "# --- NOUVELLE Fonction : Analyse sur Fenêtres Glissantes ---\n",
        "def analyze_sliding_windows(lotto_data_asc, window_size=100, step_size=20):\n",
        "    if not RUN_SLIDING_WINDOW_ENHANCED_METRICS:\n",
        "        print(\"Analyse sur fenêtres glissantes (métriques étendues) désactivée.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*40)\n",
        "    print(f\" ANALYSE SUR FENÊTRES GLISSANTES (taille={window_size}, pas={step_size})\")\n",
        "    print(f\"=\"*40)\n",
        "    if lotto_data_asc.empty or len(lotto_data_asc) < window_size:\n",
        "        print(\"Pas assez de données pour l'analyse sur fenêtres glissantes.\")\n",
        "        return\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1,6)]\n",
        "    results_over_time = []\n",
        "\n",
        "    vos_paires_compagnons = set()\n",
        "    if len(MES_NUMEROS_PRINCIPAUX_JOUES) >= 2:\n",
        "        for paire in combinations(sorted(list(MES_NUMEROS_PRINCIPAUX_JOUES)), 2):\n",
        "            vos_paires_compagnons.add(paire)\n",
        "\n",
        "    for i in range(0, len(lotto_data_asc) - window_size + 1, step_size):\n",
        "        window_df = lotto_data_asc.iloc[i : i + window_size]\n",
        "        window_end_date = window_df['Date'].iloc[-1]\n",
        "\n",
        "        window_numbers_principaux_list = []\n",
        "        for _, row in window_df[num_cols].iterrows():\n",
        "            try:\n",
        "                window_numbers_principaux_list.append([int(n) for n in row.dropna() if pd.notna(n) and str(n).isdigit()])\n",
        "            except ValueError: continue\n",
        "\n",
        "        if not window_numbers_principaux_list: continue\n",
        "        all_numbers_flat_window = [num for sublist in window_numbers_principaux_list for num in sublist]\n",
        "        if not all_numbers_flat_window: continue\n",
        "        freq_window = Counter(all_numbers_flat_window)\n",
        "\n",
        "        freq_6_w = freq_window.get(6, 0)\n",
        "        freq_7_w = freq_window.get(7, 0) # Ajout pour le numéro 7 si vous le suivez\n",
        "        freq_9_w = freq_window.get(9, 0)\n",
        "        freq_30_w = freq_window.get(30, 0)\n",
        "        var_nums_w = np.var(all_numbers_flat_window) if all_numbers_flat_window else np.nan\n",
        "        std_nums_w = np.std(all_numbers_flat_window) if all_numbers_flat_window else np.nan\n",
        "\n",
        "        compagnons_specifiques_count_w = 0\n",
        "        for draw_nums in window_numbers_principaux_list:\n",
        "            for paire_specifique in vos_paires_compagnons:\n",
        "                if set(paire_specifique).issubset(set(draw_nums)):\n",
        "                    compagnons_specifiques_count_w += 1\n",
        "\n",
        "        gaps_data_w = {num: [] for num in range(1, 50)}\n",
        "        last_seen_index_w = {}\n",
        "        for current_idx_w, draw_nums_w in enumerate(window_numbers_principaux_list):\n",
        "            for num_val_w in range(1, 50):\n",
        "                if num_val_w in draw_nums_w:\n",
        "                    if num_val_w in last_seen_index_w:\n",
        "                        gaps_data_w[num_val_w].append(current_idx_w - last_seen_index_w[num_val_w])\n",
        "                    last_seen_index_w[num_val_w] = current_idx_w\n",
        "        all_gaps_flat_w = [g for gap_list in gaps_data_w.values() for g in gap_list if gap_list]\n",
        "        overall_avg_gap_w = np.mean(all_gaps_flat_w) if all_gaps_flat_w else np.nan\n",
        "        avg_sum_w = np.mean([sum(d) for d in window_numbers_principaux_list]) if window_numbers_principaux_list else np.nan\n",
        "\n",
        "        results_over_time.append({\n",
        "            'date_fin_fenetre': window_end_date,\n",
        "            'freq_6': freq_6_w, 'freq_7': freq_7_w, 'freq_9': freq_9_w, 'freq_30': freq_30_w, # freq_7 ajoutée\n",
        "            'ecart_moyen_global_fenetre': overall_avg_gap_w,\n",
        "            'somme_moyenne_fenetre': avg_sum_w,\n",
        "            'variance_numeros_fenetre': var_nums_w,\n",
        "            'std_numeros_fenetre': std_nums_w,\n",
        "            'compagnons_specifiques_count': compagnons_specifiques_count_w\n",
        "        })\n",
        "\n",
        "    if not results_over_time:\n",
        "        print(\"Aucun résultat généré par l'analyse sur fenêtres glissantes.\")\n",
        "        return\n",
        "\n",
        "    df_results_time = pd.DataFrame(results_over_time)\n",
        "    df_results_time.set_index('date_fin_fenetre', inplace=True)\n",
        "\n",
        "    print(\"\\nQuelques résultats de l'analyse sur fenêtres glissantes (début et fin) :\")\n",
        "    print(df_results_time.head(3).to_string())\n",
        "    print(\"...\")\n",
        "    print(df_results_time.tail(3).to_string())\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        num_metrics_to_plot = 5\n",
        "        fig, axes = plt.subplots(num_metrics_to_plot, 1, figsize=(15, num_metrics_to_plot * 4), sharex=True)\n",
        "        # Ajustement pour inclure freq_7 si elle est pertinente\n",
        "        freq_cols_to_plot = ['freq_6', 'freq_9', 'freq_30']\n",
        "        if 'freq_7' in df_results_time.columns: # Vérifier si la colonne existe\n",
        "            freq_cols_to_plot.append('freq_7')\n",
        "        df_results_time[freq_cols_to_plot].plot(ax=axes[0], marker='o', linestyle='-')\n",
        "\n",
        "        axes[0].set_title('Fréquence de vos numéros sur fenêtres glissantes')\n",
        "        axes[0].set_ylabel('Fréquence'); axes[0].grid(True); axes[0].legend()\n",
        "\n",
        "        df_results_time['ecart_moyen_global_fenetre'].plot(ax=axes[1], marker='o', linestyle='-', color='green')\n",
        "        axes[1].axhline(y=8.8, color='r', linestyle='--', label='Ecart théorique (8.8)')\n",
        "        axes[1].set_title('Écart moyen global sur fenêtres glissantes')\n",
        "        axes[1].set_ylabel('Écart Moyen'); axes[1].legend(); axes[1].grid(True)\n",
        "\n",
        "        df_results_time['somme_moyenne_fenetre'].plot(ax=axes[2], marker='o', linestyle='-', color='purple')\n",
        "        axes[2].axhline(y=125, color='r', linestyle='--', label='Somme théorique moyenne (approx 125)')\n",
        "        axes[2].set_title('Somme moyenne des numéros sur fenêtres glissantes')\n",
        "        axes[2].set_ylabel('Somme Moyenne'); axes[2].legend(); axes[2].grid(True)\n",
        "\n",
        "        df_results_time['variance_numeros_fenetre'].plot(ax=axes[3], marker='o', linestyle='-', color='orange')\n",
        "        axes[3].set_title('Variance des numéros sur fenêtres glissantes')\n",
        "        axes[3].set_ylabel('Variance'); axes[3].grid(True); axes[3].legend()\n",
        "\n",
        "        df_results_time['compagnons_specifiques_count'].plot(ax=axes[4], marker='o', linestyle='-', color='brown')\n",
        "        axes[4].set_title('Nombre de vos paires de compagnons par fenêtre')\n",
        "        axes[4].set_ylabel('Compte'); axes[4].grid(True); axes[4].legend()\n",
        "\n",
        "        plt.xlabel('Date de Fin de Fenêtre'); plt.tight_layout()\n",
        "        os.makedirs(\"plots\", exist_ok=True); plt.savefig(\"plots/analyse_fenetres_glissantes_etendue.png\"); plt.close()\n",
        "        print(\"Graphique de l'analyse étendue sur fenêtres glissantes sauvegardé.\")\n",
        "\n",
        "        if RUN_SLIDING_WINDOW_TIMESERIES_ANALYSIS:\n",
        "            print(\"\\n--- Analyse ACF/PACF sur métriques de fenêtres glissantes ---\")\n",
        "            metrics_to_analyze_ts = ['ecart_moyen_global_fenetre', 'somme_moyenne_fenetre', 'variance_numeros_fenetre', 'compagnons_specifiques_count']\n",
        "            for metric_col in metrics_to_analyze_ts:\n",
        "                if metric_col in df_results_time.columns and df_results_time[metric_col].notna().sum() > 21: # Assez de points pour ACF/PACF\n",
        "                    series_for_acf = df_results_time[metric_col].dropna()\n",
        "                    if len(series_for_acf) > 3: # Minimum pour tracer\n",
        "                        try:\n",
        "                            fig_ts, axes_ts = plt.subplots(1,2,figsize=(12,4))\n",
        "                            lags_acf_pacf = min(21, len(series_for_acf)//2 -1) if len(series_for_acf)//2 -1 > 0 else 1\n",
        "\n",
        "                            sm.graphics.tsa.plot_acf(series_for_acf, lags=lags_acf_pacf, ax=axes_ts[0])\n",
        "                            sm.graphics.tsa.plot_pacf(series_for_acf, lags=lags_acf_pacf, ax=axes_ts[1], method='ywm')\n",
        "                            fig_ts.suptitle(f'ACF et PACF pour {metric_col}')\n",
        "                            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "                            plt.savefig(f\"plots/acf_pacf_{metric_col}.png\"); plt.close()\n",
        "                            print(f\"  Graphique ACF/PACF pour '{metric_col}' sauvegardé.\")\n",
        "                        except Exception as e_ts:\n",
        "                            print(f\"  Erreur lors de l'analyse ACF/PACF pour '{metric_col}': {e_ts}\")\n",
        "                    else:\n",
        "                        print(f\"  Pas assez de données pour ACF/PACF de '{metric_col}' après dropna ({len(series_for_acf)} points).\")\n",
        "\n",
        "\n",
        "            # Appel à la nouvelle fonction de modélisation\n",
        "            analyze_timeseries_metric_models(df_results_time.copy()) # Utiliser une copie pour éviter modifications\n",
        "\n",
        "\n",
        "# --- Le reste de votre script continue ici...\n",
        "# --- NOUVELLE Fonction : Analyse des Transitions ---\n",
        "def analyze_transitions(lotto_data_asc, lag=1, top_n=10):\n",
        "    if not RUN_TRANSITION_PROBABILITIES:\n",
        "        print(\"Analyse des transitions désactivée.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*40); print(f\" ANALYSE DES TRANSITIONS DE NUMÉROS (Lag={lag}, Top {top_n})\"); print(f\"=\"*40)\n",
        "    if lotto_data_asc.empty or len(lotto_data_asc) <= lag: print(\"Pas assez de données pour l'analyse des transitions.\"); return None\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, 6)]; transitions = Counter()\n",
        "    from_counts = Counter()\n",
        "\n",
        "    for i in range(len(lotto_data_asc) - lag):\n",
        "        current_draw_nums_s = lotto_data_asc.iloc[i][num_cols].dropna()\n",
        "        next_draw_nums_s = lotto_data_asc.iloc[i + lag][num_cols].dropna()\n",
        "        try:\n",
        "            current_draw_nums = {int(n) for n in current_draw_nums_s}\n",
        "            next_draw_nums = {int(n) for n in next_draw_nums_s}\n",
        "        except ValueError: continue\n",
        "\n",
        "        for num1 in current_draw_nums:\n",
        "            from_counts[num1] += 1\n",
        "            for num2 in next_draw_nums:\n",
        "                transitions[(num1, num2)] += 1\n",
        "\n",
        "    if not transitions: print(\"Aucune transition trouvée.\"); return None\n",
        "\n",
        "    transition_data = []\n",
        "    for (num_from, num_to), count in transitions.items():\n",
        "        prob_transition = count / from_counts[num_from] if from_counts[num_from] > 0 else 0\n",
        "        transition_data.append({'From': num_from, 'To': num_to, 'Count': count, 'Prob_To_Given_From': prob_transition})\n",
        "\n",
        "    df_transitions = pd.DataFrame(transition_data)\n",
        "    df_transitions.sort_values('Count', ascending=False, inplace=True)\n",
        "\n",
        "    print(f\"Top {top_n} transitions les plus fréquentes (Numéro -> Numéro après {lag} tirage(s)):\")\n",
        "    df_display = df_transitions.head(top_n).copy()\n",
        "    df_display['P(To|From)'] = df_display['Prob_To_Given_From'].map('{:.4f}'.format)\n",
        "    df_display['P(To)_baseline'] = (5/49)\n",
        "    df_display['P(To)_baseline'] = df_display['P(To)_baseline'].map('{:.4f}'.format)\n",
        "    print(df_display[['From', 'To', 'Count', 'P(To|From)', 'P(To)_baseline']].to_string(index=False))\n",
        "\n",
        "\n",
        "    print(f\"\\nTransitions les plus fréquentes depuis VOS NUMÉROS ({MES_NUMEROS_PRINCIPAUX_JOUES}):\")\n",
        "    for num_perso in MES_NUMEROS_PRINCIPAUX_JOUES:\n",
        "        top_transitions_from_perso = df_transitions[df_transitions['From'] == num_perso].sort_values('Count', ascending=False).head(min(5, top_n))\n",
        "        if not top_transitions_from_perso.empty:\n",
        "            print(f\"  Depuis {num_perso}:\")\n",
        "            for _, row_trans in top_transitions_from_perso.iterrows():\n",
        "                print(f\"     -> {row_trans['To']}: {row_trans['Count']} fois (P={row_trans['Prob_To_Given_From']:.4f}, Baseline P(To)={(5/49):.4f})\")\n",
        "        else:\n",
        "            print(f\"  Aucune transition observée depuis {num_perso} avec les filtres actuels.\")\n",
        "\n",
        "    return df_transitions\n",
        "\n",
        "\n",
        "# --- NOUVELLE Fonction : Clustering des Tirages ---\n",
        "def get_draw_features(lotto_data, config):\n",
        "    num_cols = [f\"N{i}\" for i in range(1, config[\"numbers_to_choose\"] + 1)]\n",
        "    draw_features = []\n",
        "    original_indices = []\n",
        "\n",
        "    for index, row in lotto_data.iterrows():\n",
        "        try:\n",
        "            numbers = sorted([int(row[n]) for n in num_cols if pd.notna(row[n])])\n",
        "            if len(numbers) != config[\"numbers_to_choose\"]: continue\n",
        "\n",
        "            features = [\n",
        "                sum(numbers), sum(1 for n in numbers if n % 2 == 0), numbers[0], numbers[-1], numbers[-1] - numbers[0],\n",
        "                sum(1 for n in numbers if n <= 10), sum(1 for n in numbers if 11 <= n <= 20),\n",
        "                sum(1 for n in numbers if 21 <= n <= 30), sum(1 for n in numbers if 31 <= n <= 40),\n",
        "                sum(1 for n in numbers if 41 <= n <= 49),\n",
        "            ]\n",
        "            has_seq2, has_seq3 = 0, 0\n",
        "            for i_seq in range(len(numbers) - 1):\n",
        "                if numbers[i_seq+1] - numbers[i_seq] == 1:\n",
        "                    has_seq2 = 1\n",
        "                    if i_seq + 2 < len(numbers) and numbers[i_seq+2] - numbers[i_seq+1] == 1:\n",
        "                        has_seq3 = 1; break\n",
        "            features.extend([has_seq2, has_seq3])\n",
        "            draw_features.append(features)\n",
        "            original_indices.append(index)\n",
        "        except (ValueError, TypeError): continue\n",
        "\n",
        "    df_out = pd.DataFrame(draw_features, columns=[\n",
        "        'somme', 'nb_pairs', 'min_num', 'max_num', 'etendue',\n",
        "        'diz_1_10', 'diz_11_20', 'diz_21_30', 'diz_31_40', 'diz_41_49',\n",
        "        'has_seq2', 'has_seq3'\n",
        "    ])\n",
        "    df_out.index = original_indices\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def cluster_draws(lotto_data_asc, config, n_clusters_kmeans=5):\n",
        "    if not RUN_DRAW_CLUSTERING: print(\"Clustering de tirages désactivé.\"); return None\n",
        "    print(f\"\\n\" + \"=\"*40); print(f\" CLUSTERING DES TIRAGES (K-Means avec {n_clusters_kmeans} clusters)\"); print(f\"=\"*40)\n",
        "\n",
        "    if lotto_data_asc.empty or len(lotto_data_asc) < n_clusters_kmeans * 5:\n",
        "        print(\"Pas assez de données pour le clustering.\"); return None\n",
        "\n",
        "    df_features = get_draw_features(lotto_data_asc, config)\n",
        "    if df_features.empty: print(\"Aucune feature extraite pour le clustering.\"); return None\n",
        "\n",
        "    df_features.dropna(inplace=True)\n",
        "    if df_features.empty: print(\"Aucune feature valide après nettoyage pour le clustering.\"); return None\n",
        "\n",
        "    scaler = StandardScaler(); scaled_features = scaler.fit_transform(df_features)\n",
        "    lotto_data_asc_clustered = lotto_data_asc.loc[df_features.index].copy()\n",
        "\n",
        "    try:\n",
        "        kmeans = KMeans(n_clusters=n_clusters_kmeans, random_state=42, n_init='auto')\n",
        "        lotto_data_asc_clustered['cluster_kmeans'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "        print(f\"\\nRésultats K-Means:\\n{lotto_data_asc_clustered['cluster_kmeans'].value_counts().sort_index()}\")\n",
        "        print(\"\\nMoyenne des features par cluster K-Means:\")\n",
        "\n",
        "        df_features_for_groupby = df_features.copy()\n",
        "        df_features_for_groupby['cluster_kmeans'] = lotto_data_asc_clustered['cluster_kmeans'].values\n",
        "        feature_cols_for_mean = [col for col in df_features.columns if col != 'cluster_kmeans']\n",
        "        print(df_features_for_groupby.groupby('cluster_kmeans')[feature_cols_for_mean].mean().round(2).to_string())\n",
        "\n",
        "        if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS and len(scaled_features) > 2:\n",
        "            from sklearn.decomposition import PCA\n",
        "            pca = PCA(n_components=2, random_state=42)\n",
        "            reduced_features = pca.fit_transform(scaled_features)\n",
        "            plt.figure(figsize=(10, 7))\n",
        "            sns.scatterplot(x=reduced_features[:,0], y=reduced_features[:,1],\n",
        "                            hue=lotto_data_asc_clustered['cluster_kmeans'].values,\n",
        "                            palette='viridis', s=50, alpha=0.7)\n",
        "            plt.title(f'Clusters de Tirages (K-Means via PCA)'); plt.xlabel('Composante Principale 1'); plt.ylabel('Composante Principale 2')\n",
        "            plt.legend(title='Cluster K-Means'); plt.grid(True); plt.tight_layout()\n",
        "            plt.savefig(\"plots/draw_clusters_kmeans_pca.png\"); plt.close()\n",
        "            print(\"Graphique clustering K-Means (PCA) sauvegardé.\")\n",
        "    except Exception as e_kmeans:\n",
        "        print(f\"Erreur K-Means: {e_kmeans}\")\n",
        "        if 'cluster_kmeans' in lotto_data_asc_clustered.columns: lotto_data_asc_clustered.drop('cluster_kmeans', axis=1, inplace=True)\n",
        "\n",
        "    return lotto_data_asc_clustered\n",
        "\n",
        "# --- NOUVELLE FONCTION : Analyse des séquences fréquentes avec PrefixSpan ---\n",
        "def analyze_frequent_sequences_prefixspan(lotto_data, min_support_ratio=0.005, max_pattern_length=5):\n",
        "    \"\"\"\n",
        "    Analyse les séquences de numéros fréquents dans les tirages du loto en utilisant PrefixSpan.\n",
        "\n",
        "    Args:\n",
        "        lotto_data (pd.DataFrame): DataFrame contenant les tirages du loto (colonnes N1 à N5).\n",
        "        min_support_ratio (float): Ratio de support minimum (ex: 0.01 pour 1% des tirages).\n",
        "        max_pattern_length (int): Longueur maximale des séquences à afficher.\n",
        "    \"\"\"\n",
        "    if not isinstance(lotto_data, pd.DataFrame) or lotto_data.empty:\n",
        "        print(\"Aucune donnée valide pour l'analyse PrefixSpan.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*40)\n",
        "    print(f\" ANALYSE DES SÉQUENCES FRÉQUENTES AVEC PREFIXSPAN\")\n",
        "    print(f\" Support Minimum: {min_support_ratio*100:.2f}% des tirages\")\n",
        "    print(f\" Longueur Maximale de Séquence: {max_pattern_length}\")\n",
        "    print(f\"=\"*40)\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, 6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols):\n",
        "        print(\"Colonnes N1-N5 manquantes pour l'analyse PrefixSpan.\")\n",
        "        return\n",
        "\n",
        "    sequences = []\n",
        "    for _, row in lotto_data.iterrows():\n",
        "        try:\n",
        "            # Assurez-vous que les numéros sont des entiers et triés pour des séquences cohérentes\n",
        "            current_draw_nums = sorted([int(n) for n in row[num_cols].dropna() if pd.notna(n) and str(n).isdigit()])\n",
        "            if len(current_draw_nums) == 5: # Ne considérer que les tirages complets de 5 numéros\n",
        "                sequences.append(current_draw_nums)\n",
        "        except ValueError:\n",
        "            logging.warning(f\"Ligne avec numéros non convertibles (PrefixSpan): {row.tolist()}. Ignorée.\")\n",
        "            continue\n",
        "\n",
        "    if not sequences:\n",
        "        print(\"Aucune séquence valide extraite pour l'analyse PrefixSpan.\")\n",
        "        return\n",
        "\n",
        "    # Calculer min_support en nombre absolu de séquences\n",
        "    num_sequences = len(sequences)\n",
        "    min_support_count = int(num_sequences * min_support_ratio)\n",
        "    if min_support_count == 0 and num_sequences > 0: # S'assurer d'au moins 1 si le ratio est très petit\n",
        "        min_support_count = 1\n",
        "\n",
        "    print(f\"Nombre total de tirages (séquences) pour PrefixSpan: {num_sequences}\")\n",
        "    print(f\"Nombre minimum de support requis: {min_support_count} tirages\")\n",
        "\n",
        "    try:\n",
        "        ps = PrefixSpan(sequences)\n",
        "        # La méthode 'frequent' prend le support minimum directement comme premier argument.\n",
        "        all_frequent_patterns_with_support = ps.frequent(min_support_count)\n",
        "\n",
        "        # Trier les motifs par support (fréquence) dans l'ordre décroissant\n",
        "        all_frequent_patterns_with_support.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        print(\"\\nSéquences fréquentes (Support, Séquence):\")\n",
        "        found_patterns = False\n",
        "        for support, pattern in all_frequent_patterns_with_support:\n",
        "            # Filtrer par la longueur maximale souhaitée\n",
        "            if len(pattern) <= max_pattern_length:\n",
        "                print(f\"  Support: {support} ({support/num_sequences*100:.2f}%), Séquence: {pattern}\")\n",
        "                found_patterns = True\n",
        "        if not found_patterns:\n",
        "            print(\"Aucune séquence fréquente trouvée avec les critères donnés.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erreur lors de l'exécution de PrefixSpan: {e}\", exc_info=True)\n",
        "        print(f\"Une erreur est survenue lors de l'analyse PrefixSpan: {e}\")\n",
        "\n",
        "    print(f\"=\"*40)\n",
        "\n",
        "# --- Fonctions d'Analyse Statistique (existantes) ---\n",
        "def analyze_frequencies(lotto_data, title=\"Générale\", save_prefix=\"general\"):\n",
        "    if lotto_data.empty:\n",
        "        print(f\"Aucune donnée pour analyse de fréquence '{title}'.\")\n",
        "        return None\n",
        "    num_cols = [f\"N{i}\" for i in range(1,6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols):\n",
        "        print(f\"Colonnes N1-N5 manquantes pour l'analyse de fréquence '{title}'.\")\n",
        "        return None\n",
        "\n",
        "    valid_data = lotto_data[num_cols].dropna().copy()\n",
        "    for col in num_cols: valid_data[col] = pd.to_numeric(valid_data[col], errors='coerce')\n",
        "    all_numbers = valid_data.stack().dropna().astype(int).values\n",
        "\n",
        "    if not all_numbers.size:\n",
        "        print(f\"Aucun numéro trouvé pour l'analyse de fréquence '{title}'.\")\n",
        "        return None\n",
        "\n",
        "    max_num = 49\n",
        "    freq = pd.Series(all_numbers).value_counts().reindex(range(1, max_num + 1), fill_value=0)\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(f\"\\n--- Fréquence des numéros ({title}) ---\\nTop 10:\\n{freq.sort_values(ascending=False).head(10)}\")\n",
        "        print(\"\\nFréquences de VOS numéros joués (principaux) :\")\n",
        "        for num_perso in sorted(list(MES_NUMEROS_PRINCIPAUX_JOUES)):\n",
        "            print(f\"  Numéro {num_perso}: {freq.get(num_perso, 0)} apparitions\")\n",
        "\n",
        "    if (total_occ := len(all_numbers)) > 0 and (num_outcomes := max_num) > 0:\n",
        "        exp_freq_val = total_occ / num_outcomes\n",
        "        exp_freqs = np.full(num_outcomes, exp_freq_val)\n",
        "        obs_freqs = freq.values\n",
        "        if len(obs_freqs) == len(exp_freqs) > 1:\n",
        "            chi2, p_val = chisquare(f_obs=obs_freqs, f_exp=exp_freqs)\n",
        "            if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "                print(f\"Test chi² d'uniformité ({title}): stat={chi2:.2f}, p-val={p_val:.5f}. {'Distribution semble uniforme' if p_val >= 0.05 else 'Distribution NON uniforme (biais possible)'}\")\n",
        "        elif RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "            print(f\"Impossible d'effectuer le test chi² ({title}): longueurs incompatibles ou < 2.\")\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        plt.figure(figsize=(12,5))\n",
        "        sns.barplot(x=freq.index, y=freq.values)\n",
        "        plt.title(f\"Fréquence des Numéros ({title})\")\n",
        "        plt.xlabel(\"Numéro\"); plt.ylabel(\"Nombre d'apparitions\")\n",
        "        plt.xticks(np.arange(0, max_num + 1, 2))\n",
        "        plt.grid(axis='y',linestyle='--'); plt.tight_layout();\n",
        "        os.makedirs(\"plots\", exist_ok=True); plt.savefig(f\"plots/freq_histogram_{save_prefix}.png\"); plt.close();\n",
        "        print(f\"Histogramme des fréquences sauvegardé: plots/freq_histogram_{save_prefix}.png\")\n",
        "    return freq\n",
        "\n",
        "def analyze_frequencies_by_day(lotto_data):\n",
        "    if not RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: return\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n ANALYSE DES FRÉQUENCES PAR JOUR DE TIRAGE \\n\" + \"=\"*40)\n",
        "    if 'Jour' not in lotto_data.columns: print(\"La colonne 'Jour' est manquante.\"); return\n",
        "\n",
        "    jours_disponibles = lotto_data[\"Jour\"].astype(str).str.capitalize().unique()\n",
        "    jours_a_analyser = [j for j in ['Lundi', 'Mercredi', 'Samedi'] if j in jours_disponibles]\n",
        "    if not jours_a_analyser: print(\"Aucun des jours standards trouvé.\"); return\n",
        "\n",
        "    freq_by_day, nums_by_day = {}, {}\n",
        "    num_cols = [f\"N{i}\" for i in range(1,6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols): print(\"Colonnes N1-N5 manquantes.\"); return\n",
        "\n",
        "    for jour in jours_a_analyser:\n",
        "        data_j = lotto_data[lotto_data[\"Jour\"].astype(str).str.contains(jour, case=False, na=False)]\n",
        "        if data_j.empty: print(f\"Aucun tirage pour {jour}.\"); continue\n",
        "        if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(f\"\\nAnalyse pour le jour : {jour}\")\n",
        "        freq_by_day[jour] = analyze_frequencies(data_j, title=f\"Fréquences {jour} (données filtrées)\", save_prefix=f\"freq_{jour.lower()}_filtered\")\n",
        "        if freq_by_day[jour] is not None:\n",
        "            valid_data_j = data_j[num_cols].dropna().copy()\n",
        "            for col in num_cols: valid_data_j[col] = pd.to_numeric(valid_data_j[col], errors='coerce')\n",
        "            nums_by_day[jour] = valid_data_j.stack().dropna().astype(int).values\n",
        "\n",
        "    if len(nums_by_day) < 2: print(\"Pas assez de jours pour test d'indépendance.\"); return\n",
        "\n",
        "    print(\"\\n--- Test Chi² d'Indépendance des Distributions de Fréquences par Jour ---\")\n",
        "    all_possible_nums = range(1, 50)\n",
        "    jours_data_valides = [j for j in jours_a_analyser if j in nums_by_day and nums_by_day[j].size > 0]\n",
        "    if len(jours_data_valides) < 2: print(\"Moins de deux jours avec données suffisantes pour test.\"); return\n",
        "\n",
        "    contingency_table = pd.DataFrame(index=all_possible_nums, columns=jours_data_valides, dtype=float)\n",
        "    for num in all_possible_nums:\n",
        "        for jour in jours_data_valides:\n",
        "            contingency_table.loc[num, jour] = np.sum(nums_by_day[jour] == num)\n",
        "    contingency_table = contingency_table.fillna(0).astype(float)\n",
        "\n",
        "    observed_df_filtered = contingency_table.loc[(contingency_table.sum(axis=1) > 0)]\n",
        "    if observed_df_filtered.shape[0] < 2 or observed_df_filtered.shape[1] < 2: print(\"Table de contingence invalide après filtrage.\"); return\n",
        "\n",
        "    try:\n",
        "        chi2, p_val, dof, expected = chi2_contingency(observed_df_filtered)\n",
        "        print(f\"Résultat Test Chi² d'indépendance: Chi²={chi2:.2f}, p-valeur={p_val:.5f}, ddl={dof}.\")\n",
        "        print(f\"Conclusion: {'Distributions indépendantes du jour.' if p_val >=0.05 else 'Possible dépendance au jour.'}\")\n",
        "    except ValueError as e: print(f\"Erreur calcul Chi² d'indépendance: {e}\")\n",
        "\n",
        "    if len(freq_by_day) > 1 and RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        plt.figure(figsize=(14,7)); plotted_something = False\n",
        "        for jour, freq_data in freq_by_day.items():\n",
        "            if freq_data is not None and not freq_data.empty: sns.lineplot(x=freq_data.index, y=freq_data.values, label=jour); plotted_something=True\n",
        "        if plotted_something:\n",
        "            plt.title(\"Comparaison Fréquences par Jour\"); plt.xlabel(\"Numéro\"); plt.ylabel(\"Apparitions\")\n",
        "            plt.xticks(np.arange(1,50,2)); plt.legend(); plt.grid(axis='y',linestyle='--'); plt.tight_layout();\n",
        "            plt.savefig(\"plots/freq_comparison_by_day_filtered.png\"); plt.close();\n",
        "            print(\"Graphique comparaison fréquences par jour sauvegardé.\")\n",
        "\n",
        "def analyze_combinations(lotto_data, n_numbers=2, max_num=49, top_n=21, save_prefix=\"pairs\"):\n",
        "    if not RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS and n_numbers > 2 : return None\n",
        "    if lotto_data.empty: print(f\"Aucune donnée pour combinaisons de {n_numbers}.\"); return None\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1,6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols) or len(num_cols) < n_numbers: print(f\"Colonnes N1-N5 manquantes/insuffisantes.\"); return None\n",
        "\n",
        "    draws = []\n",
        "    for _, row in lotto_data[num_cols].iterrows():\n",
        "        try:\n",
        "            valid_numbers_in_row = [int(n) for n in row.dropna() if pd.notna(n) and str(n).isdigit()]\n",
        "            if len(valid_numbers_in_row) >= n_numbers: draws.append(valid_numbers_in_row)\n",
        "        except ValueError: logging.warning(f\"Ligne avec numéros non convertibles (combinations): {row.tolist()}\"); continue\n",
        "\n",
        "    if not draws: print(f\"Aucun tirage valide pour combinaisons de {n_numbers}.\"); return None\n",
        "\n",
        "    combos = [tuple(sorted(c)) for draw in draws for c in combinations(draw, n_numbers)]\n",
        "    if not combos: print(f\"Aucune combinaison de {n_numbers} générée.\"); return None\n",
        "\n",
        "    counts = Counter(combos)\n",
        "    df_combinations = pd.DataFrame.from_dict(counts, orient='index', columns=['Frequency']).sort_values('Frequency', ascending=False)\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(f\"\\n--- Fréquence des Combinaisons de {n_numbers} numéros ---\\nTop {top_n}:\\n{df_combinations.head(top_n).to_string()}\")\n",
        "        total_possible_combinations = nCr(max_num, n_numbers)\n",
        "        print(f\"Combinaisons possibles de {n_numbers} (sur {max_num}): {total_possible_combinations}\")\n",
        "        print(f\"Combinaisons de {n_numbers} observées: {len(combos)}\")\n",
        "        if total_possible_combinations > 0: print(f\"Fréquence moyenne attendue: {len(combos)/total_possible_combinations:.2f}\")\n",
        "\n",
        "        if not df_combinations.empty:\n",
        "            plt.figure(figsize=(max(10, int(top_n*0.4)),6))\n",
        "            top_plot_df = df_combinations.head(top_n)\n",
        "            top_plot_df.index = top_plot_df.index.map(lambda x: ', '.join(map(str,x)) if isinstance(x, tuple) else x)\n",
        "            sns.barplot(data=top_plot_df, x=top_plot_df.index, y='Frequency', hue=top_plot_df.index, palette='viridis', legend=False, dodge=False)\n",
        "            plt.title(f\"Top {top_n} Combinaisons de {n_numbers} Numéros\"); plt.xlabel(f\"Combinaison de {n_numbers}\"); plt.ylabel(\"Fréquence\")\n",
        "            plt.xticks(rotation=45, ha='right'); plt.tight_layout();\n",
        "            os.makedirs(\"plots\", exist_ok=True); plt.savefig(f\"plots/top_{top_n}_{save_prefix}_combinations.png\"); plt.close();\n",
        "            print(f\"Graphique top {top_n} {save_prefix} sauvegardé.\")\n",
        "    return df_combinations\n",
        "\n",
        "def analyze_gaps(lotto_data_asc, max_num=49):\n",
        "    if not RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: return None\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n ANALYSE DES ÉCARTS (GAPS) ENTRE APPARITIONS \\n\" + \"=\"*40)\n",
        "    if lotto_data_asc.empty: print(\"Aucune donnée pour analyse des écarts.\"); return None\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1,6)]\n",
        "    if not all(c in lotto_data_asc.columns for c in num_cols): print(\"Colonnes N1-N5 manquantes.\"); return None\n",
        "\n",
        "    draws_list = []\n",
        "    for _, row in lotto_data_asc[num_cols].iterrows():\n",
        "        try: draws_list.append([int(n) for n in row.dropna() if pd.notna(n) and str(n).isdigit()])\n",
        "        except ValueError: logging.warning(f\"Ligne avec numéros non convertibles (gaps): {row.tolist()}\"); continue\n",
        "\n",
        "    if not draws_list: print(\"Aucun tirage valide pour analyse des écarts.\"); return None\n",
        "\n",
        "    last_seen_index, gaps_data = {}, {num: [] for num in range(1, max_num + 1)}\n",
        "    for current_index, draw_numbers in enumerate(draws_list):\n",
        "        for num_val in range(1, max_num + 1):\n",
        "            if num_val in draw_numbers:\n",
        "                if num_val in last_seen_index: gaps_data[num_val].append(current_index - last_seen_index[num_val])\n",
        "                last_seen_index[num_val] = current_index\n",
        "\n",
        "    average_gaps = {num: np.mean(gap_list) if gap_list else np.nan for num, gap_list in gaps_data.items()}\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(\"Écart moyen entre les apparitions pour chaque numéro (1-49):\")\n",
        "        for num, avg_gap_val in sorted(average_gaps.items()): print(f\"  Numéro {num:2d}: {avg_gap_val:.2f} tirages\" if pd.notna(avg_gap_val) else f\"  Numéro {num:2d}: N/A\")\n",
        "\n",
        "    all_gaps_flat_list = [g for gap_list in gaps_data.values() for g in gap_list if gap_list]\n",
        "    overall_avg_gap = np.mean(all_gaps_flat_list) if all_gaps_flat_list else np.nan\n",
        "    expected_gap = (max_num / 5.0) - 1.0\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(f\"\\nÉcart moyen global observé: {overall_avg_gap:.2f} tirages.\")\n",
        "        print(f\"Écart moyen attendu (théorique): {expected_gap:.2f} tirages.\")\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        plot_numbers = sorted([n for n, avg in average_gaps.items() if pd.notna(avg)])\n",
        "        plot_values = [average_gaps[n] for n in plot_numbers]\n",
        "        if plot_numbers:\n",
        "            plt.figure(figsize=(12,5))\n",
        "            sns.barplot(x=plot_numbers, y=plot_values, color='skyblue')\n",
        "            if pd.notna(expected_gap): plt.axhline(expected_gap, color='red', linestyle='--', label=f'Écart Attendu ({expected_gap:.2f})')\n",
        "            plt.title(\"Écart Moyen Entre Apparitions par Numéro\"); plt.xlabel(\"Numéro\"); plt.ylabel(\"Écart Moyen\")\n",
        "            tick_step = max(1, len(plot_numbers) // 21)\n",
        "            plt.xticks(ticks=np.arange(0, len(plot_numbers), tick_step), labels=[plot_numbers[i] for i in np.arange(0, len(plot_numbers), tick_step).astype(int)] if plot_numbers else [])\n",
        "            plt.legend(); plt.grid(axis='y',linestyle='--'); plt.tight_layout();\n",
        "            os.makedirs(\"plots\", exist_ok=True); plt.savefig(f\"plots/average_gaps_by_number.png\"); plt.close();\n",
        "            print(\"Graphique écarts moyens par numéro sauvegardé.\")\n",
        "\n",
        "        example_numbers_for_gaps = [n for n in [1,6,7,9,13,14,19,21,25,30,31,35,37,42,43,49] if n in gaps_data and len(gaps_data[n]) > 5]\n",
        "        if example_numbers_for_gaps:\n",
        "            num_examples = len(example_numbers_for_gaps); ncols_plot = min(3, num_examples); nrows_plot = (num_examples + ncols_plot - 1) // ncols_plot\n",
        "            if nrows_plot > 0:\n",
        "                fig, axes_array = plt.subplots(nrows_plot, ncols_plot, figsize=(ncols_plot*4, nrows_plot*3.5), squeeze=False)\n",
        "                axes_flat = axes_array.flatten()\n",
        "                for i, num_example in enumerate(example_numbers_for_gaps):\n",
        "                    if i < len(axes_flat) and gaps_data[num_example]:\n",
        "                        sns.histplot(gaps_data[num_example], bins=max(1, (max(gaps_data[num_example]) if gaps_data[num_example] else 1)//2), kde=False, ax=axes_flat[i])\n",
        "                        axes_flat[i].set_title(f\"Distribution Écarts N°{num_example}\"); axes_flat[i].set_xlabel(\"Écart\"); axes_flat[i].set_ylabel(\"Fréquence\"); axes_flat[i].grid(axis='y',linestyle='--')\n",
        "                plt.tight_layout(); os.makedirs(\"plots\", exist_ok=True); plt.savefig(\"plots/gap_distributions_examples.png\"); plt.close();\n",
        "                print(\"Graphique distributions d'écarts (exemples) sauvegardé.\")\n",
        "    return gaps_data\n",
        "\n",
        "def analyze_temporal_correlation(lotto_data_asc, max_lag=10, max_num=49):\n",
        "    if not RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: return None\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n ANALYSE DE L'AUTOCORRÉLATION TEMPORELLE \\n\" + \"=\"*40)\n",
        "    if lotto_data_asc.empty: print(\"Aucune donnée pour corrélation temporelle.\"); return None\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1,6)];\n",
        "    if not all(c in lotto_data_asc.columns for c in num_cols): print(\"Colonnes N1-N5 manquantes.\"); return None\n",
        "\n",
        "    draws_list_int = []\n",
        "    for _, row in lotto_data_asc[num_cols].iterrows():\n",
        "        try: draws_list_int.append([int(n) for n in row.dropna() if pd.notna(n) and str(n).isdigit() and 1 <= int(n) <= max_num])\n",
        "        except ValueError: logging.warning(f\"Ligne avec numéros non convertibles (correlation): {row.tolist()}\"); continue\n",
        "\n",
        "    num_draws = len(draws_list_int)\n",
        "    if num_draws <= max_lag: print(f\"Pas assez de tirages ({num_draws}) pour corrélation (lag max {max_lag}).\"); return {}\n",
        "\n",
        "    binary_occurrence_matrix = np.zeros((num_draws, max_num), dtype=int)\n",
        "    for i, draw_numbers in enumerate(draws_list_int):\n",
        "        for num_val in draw_numbers:\n",
        "            if 1 <= num_val <= max_num: binary_occurrence_matrix[i, num_val - 1] = 1\n",
        "\n",
        "    autocorrelations_results = {num: {} for num in range(1, max_num + 1)}\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(\"Calcul de l'autocorrélation (affichage partiel):\")\n",
        "    printed_examples_count = 0\n",
        "\n",
        "    for num_to_analyze in range(1, max_num + 1):\n",
        "        time_series_for_num = pd.Series(binary_occurrence_matrix[:, num_to_analyze - 1])\n",
        "        if time_series_for_num.nunique() > 1 and len(time_series_for_num) > max_lag:\n",
        "            for lag_val in range(1, max_lag + 1):\n",
        "                if time_series_for_num.var() != 0:\n",
        "                    try:\n",
        "                        correlation_value = time_series_for_num.autocorr(lag=lag_val)\n",
        "                        if isinstance(correlation_value, float) and not np.isnan(correlation_value): autocorrelations_results[num_to_analyze][lag_val] = correlation_value\n",
        "                    except Exception as e_autocorr: logging.debug(f\"Erreur autocorr N°{num_to_analyze}, lag {lag_val}: {e_autocorr}\")\n",
        "\n",
        "            if autocorrelations_results[num_to_analyze] and printed_examples_count < 5 and RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "                formatted_corrs = {lag: f\"{corr:.3f}\" for lag, corr in list(autocorrelations_results[num_to_analyze].items())[:3]}\n",
        "                print(f\"  Numéro {num_to_analyze}: {formatted_corrs} ...\"); printed_examples_count += 1\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        example_nums_for_autocorr = [n for n in [1,6,7,9,13,14,19,21,25,30,31,35,37,42,43,49] if n in autocorrelations_results and autocorrelations_results[n]]\n",
        "        if example_nums_for_autocorr and num_draws > 1:\n",
        "            num_examples_plot = len(example_nums_for_autocorr); ncols_plot_ac = min(3, num_examples_plot); nrows_plot_ac = (num_examples_plot + ncols_plot_ac - 1) // ncols_plot_ac\n",
        "            if nrows_plot_ac > 0:\n",
        "                fig_ac, axes_ac_array = plt.subplots(nrows_plot_ac, ncols_plot_ac, figsize=(ncols_plot_ac*4, nrows_plot_ac*3.5), squeeze=False)\n",
        "                axes_ac_flat = axes_ac_array.flatten(); confidence_limit = 1.96 / np.sqrt(num_draws)\n",
        "\n",
        "                for i, num_plot_ac in enumerate(example_nums_for_autocorr):\n",
        "                    if i < len(axes_ac_flat) and autocorrelations_results[num_plot_ac]:\n",
        "                        lags_plot = sorted(autocorrelations_results[num_plot_ac].keys()); corrs_plot = [autocorrelations_results[num_plot_ac][l] for l in lags_plot]\n",
        "                        current_ax = axes_ac_flat[i]; current_ax.bar(lags_plot, corrs_plot, color='cornflowerblue')\n",
        "                        current_ax.axhline(0, color='grey', linewidth=0.8); current_ax.axhline(confidence_limit, color='red', linestyle='--', linewidth=0.8, label='Limite Signif. (95%)'); current_ax.axhline(-confidence_limit, color='red', linestyle='--', linewidth=0.8)\n",
        "                        current_ax.set_title(f\"Autocorrélation N°{num_plot_ac}\"); current_ax.set_xlabel(\"Lag\"); current_ax.set_ylabel(\"Coeff. Autocorr.\")\n",
        "                        current_ax.set_xticks(lags_plot);\n",
        "                        if i == 0: current_ax.legend(fontsize='small')\n",
        "                        current_ax.grid(axis='y', linestyle='--')\n",
        "                plt.tight_layout(); os.makedirs(\"plots\", exist_ok=True); plt.savefig(\"plots/autocorrelation_by_number_examples.png\"); plt.close();\n",
        "                print(\"Graphique exemples autocorrélation sauvegardé.\")\n",
        "    return autocorrelations_results\n",
        "\n",
        "def analyser_forme_des_tirages(lotto_data, save_prefix=\"current_dataset_shape\"):\n",
        "    if not RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: return\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n ANALYSE DE LA FORME DES TIRAGES \\n\" + \"=\"*40)\n",
        "    if lotto_data.empty: print(\"Aucune donnée pour analyse de forme.\"); return\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, 6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols): print(\"Colonnes N1-N5 manquantes.\"); return\n",
        "\n",
        "    try: lotto_numbers_df = lotto_data[num_cols].dropna().astype(int)\n",
        "    except ValueError: logging.error(\"Erreur conversion int (analyser_forme).\"); print(\"Erreur conversion (forme).\"); return\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(\"\\n--- Analyse Pairs/Impairs ---\")\n",
        "    pairs_counts = lotto_numbers_df.apply(lambda row: sum(n % 2 == 0 for n in row), axis=1); impairs_counts = 5 - pairs_counts\n",
        "    pair_impair_dist = pd.DataFrame({'pairs': pairs_counts, 'impairs': impairs_counts}).value_counts().sort_index()\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(f\"Distribution Pairs/Impairs:\\n{pair_impair_dist}\")\n",
        "    if not pair_impair_dist.empty and RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        pair_impair_dist.plot(kind='bar', figsize=(10, 6)); plt.title(\"Distribution Pairs/Impairs\"); plt.xlabel(\"Combinaison (Pairs, Impairs)\"); plt.ylabel(\"Nb Tirages\")\n",
        "        plt.xticks(rotation=45); plt.grid(axis='y', linestyle='--'); plt.tight_layout(); plt.savefig(f\"plots/{save_prefix}_pairs_impairs.png\"); plt.close(); print(f\"Graphique Pairs/Impairs sauvegardé.\")\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(\"\\n--- Analyse Somme Numéros Principaux ---\")\n",
        "    sommes_tirages = lotto_numbers_df.sum(axis=1)\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(f\"Stats sommes:\\n{sommes_tirages.describe().to_string()}\")\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        plt.figure(figsize=(10, 6)); sns.histplot(sommes_tirages, kde=True, bins=30); plt.title(\"Distribution Somme Numéros Principaux\"); plt.xlabel(\"Somme 5 Numéros\"); plt.ylabel(\"Fréquence\")\n",
        "        plt.grid(axis='y', linestyle='--'); plt.tight_layout(); plt.savefig(f\"plots/{save_prefix}_sommes_tirages.png\"); plt.close(); print(f\"Graphique sommes sauvegardé.\")\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(\"\\n--- Répartition Numéros par Dizaines ---\")\n",
        "    dizaines_bins = [0, 9, 19, 29, 39, 49]; dizaines_labels = ['1-9', '10-19', '20-29', '30-39', '40-49']\n",
        "    repartition_dizaines_counts = pd.DataFrame(columns=dizaines_labels, dtype=int)\n",
        "    for _, row in lotto_numbers_df.iterrows():\n",
        "        counts_par_dizaine_row = pd.cut(row, bins=dizaines_bins, labels=dizaines_labels, right=True, include_lowest=True).value_counts().reindex(dizaines_labels, fill_value=0)\n",
        "        repartition_dizaines_counts = pd.concat([repartition_dizaines_counts, counts_par_dizaine_row.to_frame().T.astype(int)], ignore_index=True)\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(f\"Nombre moyen de numéros par dizaine:\\n{repartition_dizaines_counts.mean()}\")\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        plt.figure(figsize=(12, 7)); sns.boxplot(data=repartition_dizaines_counts); plt.title(\"Distribution Nb Numéros par Dizaine\"); plt.xlabel(\"Dizaine\"); plt.ylabel(\"Nb Numéros Dizaine\")\n",
        "        plt.grid(axis='y', linestyle='--'); plt.tight_layout(); plt.savefig(f\"plots/{save_prefix}_repartition_dizaines.png\"); plt.close(); print(f\"Graphique répartitions dizaines sauvegardé.\")\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: print(\"\\n--- Analyse Séquences Numéros ---\")\n",
        "    sequences_counts = {2:0, 3:0, 4:0, 5:0}\n",
        "    for _, row in lotto_numbers_df.iterrows():\n",
        "        sorted_numbers = sorted(list(row)); found_max_seq_in_row = 0\n",
        "        for i in range(len(sorted_numbers)):\n",
        "            current_seq_len = 1\n",
        "            for j in range(i + 1, len(sorted_numbers)):\n",
        "                if sorted_numbers[j] - sorted_numbers[j-1] == 1: current_seq_len += 1\n",
        "                else: break\n",
        "            if current_seq_len >= 2: found_max_seq_in_row = max(found_max_seq_in_row, current_seq_len)\n",
        "        if found_max_seq_in_row >= 2: sequences_counts[found_max_seq_in_row] +=1\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(\"Nb tirages avec séquence maximale de longueur X:\")\n",
        "        for length, count in sequences_counts.items():\n",
        "            if count > 0 : print(f\"  Séquences max longueur {length}: {count} tirages\")\n",
        "    if any(sequences_counts.values()) and RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        seq_series = pd.Series(sequences_counts); seq_series_plot = seq_series[seq_series > 0]\n",
        "        if not seq_series_plot.empty:\n",
        "            seq_series_plot.plot(kind='bar', figsize=(8,5)); plt.title(\"Nb Tirages Séquences Numériques (longueur max)\"); plt.xlabel(\"Longueur Max Séquence\"); plt.ylabel(\"Nb Tirages\")\n",
        "            plt.xticks(rotation=0); plt.grid(axis='y', linestyle='--'); plt.tight_layout(); plt.savefig(f\"plots/{save_prefix}_sequences_max_par_tirage.png\"); plt.close(); print(f\"Graphique séquences sauvegardé.\")\n",
        "\n",
        "\n",
        "def analyser_numeros_compagnons(lotto_data, top_n_compagnons=5, save_prefix=\"current_dataset_companions\"):\n",
        "    if not RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS: return\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n ANALYSE DES NUMÉROS COMPAGNONS \\n\" + \"=\"*40)\n",
        "    if lotto_data.empty: print(\"Aucune donnée pour analyse compagnons.\"); return\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, 6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols): print(\"Colonnes N1-N5 manquantes.\"); return\n",
        "\n",
        "    try: lotto_numbers_df = lotto_data[num_cols].dropna().astype(int)\n",
        "    except ValueError: logging.error(\"Erreur conversion int (analyser_compagnons).\"); print(\"Erreur conversion (compagnons).\"); return\n",
        "\n",
        "    compagnons_counts = Counter()\n",
        "    for _, row in lotto_numbers_df.iterrows():\n",
        "        for pair in combinations(sorted(list(row)), 2): compagnons_counts[pair] += 1\n",
        "\n",
        "    if not compagnons_counts: print(\"Aucune paire de compagnons trouvée.\"); return\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(f\"Top {top_n_compagnons * 5} paires de numéros sortant le plus ensemble:\")\n",
        "        for i, (pair, count) in enumerate(compagnons_counts.most_common(top_n_compagnons * 5)):\n",
        "            print(f\"  Paire {pair}: {count} fois\");\n",
        "            if i >= (top_n_compagnons * 5 -1) : break\n",
        "\n",
        "    num_compagnons_details = {num: Counter() for num in range(1, 50)}\n",
        "    for pair, count in compagnons_counts.items():\n",
        "        num_compagnons_details[pair[0]][pair[1]] += count; num_compagnons_details[pair[1]][pair[0]] += count\n",
        "\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        print(f\"\\nTop {top_n_compagnons} compagnons pour VOS numéros joués ({MES_NUMEROS_PRINCIPAUX_JOUES}):\")\n",
        "        for num_principal in sorted(list(MES_NUMEROS_PRINCIPAUX_JOUES)):\n",
        "            if num_principal in num_compagnons_details and num_compagnons_details[num_principal]:\n",
        "                top_comp = num_compagnons_details[num_principal].most_common(top_n_compagnons)\n",
        "                print(f\"  Pour votre numéro {num_principal}: {top_comp}\")\n",
        "            else: print(f\"  Pas de données compagnons pour N°{num_principal}.\")\n",
        "\n",
        "def run_full_statistical_analysis(lotto_data_asc):\n",
        "    if lotto_data_asc is None or lotto_data_asc.empty:\n",
        "        print(\"Pas de données pour l'analyse statistique complète.\")\n",
        "        return {}\n",
        "    print(f\"\\nLancement de l'analyse statistique (détaillée: {RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS}) sur {len(lotto_data_asc)} tirages.\")\n",
        "    analysis_results = {}\n",
        "    analysis_results['frequencies'] = analyze_frequencies(lotto_data_asc.copy(), title=\"Fréquences (Dataset Actuel)\", save_prefix=\"current_dataset_freq\")\n",
        "    if RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS:\n",
        "        analyze_frequencies_by_day(lotto_data_asc.copy())\n",
        "        analysis_results['pairs'] = analyze_combinations(lotto_data_asc.copy(), n_numbers=2, top_n=20, save_prefix=\"pairs_current_dataset\")\n",
        "        analysis_results['triplets'] = analyze_combinations(lotto_data_asc.copy(), n_numbers=3, top_n=10, save_prefix=\"triplets_current_dataset\")\n",
        "        analysis_results['gaps'] = analyze_gaps(lotto_data_asc.copy())\n",
        "        analysis_results['temporal_correlation'] = analyze_temporal_correlation(lotto_data_asc.copy(), max_lag=10)\n",
        "        analyser_forme_des_tirages(lotto_data_asc.copy(), save_prefix=\"current_dataset_shape\")\n",
        "        analyser_numeros_compagnons(lotto_data_asc.copy(), save_prefix=\"current_dataset_companions\")\n",
        "    else:\n",
        "        print(\"Analyses statistiques détaillées et plots désactivés (RUN_FULL_STAT_ANALYSIS_VERBOSE_PLOTS=False).\")\n",
        "    print(\"\\nAnalyse statistique terminée.\")\n",
        "    return analysis_results\n",
        "\n",
        "def calculer_statistiques_avancees(historique_df_asc):\n",
        "    stats = {}\n",
        "    nombre_total_tirages = len(historique_df_asc)\n",
        "    if nombre_total_tirages == 0: return stats\n",
        "\n",
        "    max_num_principal, max_num_comp = 49, 10\n",
        "\n",
        "    for num in range(1, max_num_principal + 1):\n",
        "        stats[num] = {'type': 'principal', 'valeur_reelle': num}\n",
        "        try:\n",
        "            apparitions = (historique_df_asc[['N1','N2','N3','N4','N5']].apply(pd.to_numeric, errors='coerce') == num).any(axis=1)\n",
        "            indices_apparitions = historique_df_asc[apparitions].index.tolist()\n",
        "        except Exception as e: logging.error(f\"Erreur calcul apparitions N°P {num}: {e}\"); indices_apparitions = []\n",
        "\n",
        "        stats[num]['frequence_absolue'] = len(indices_apparitions)\n",
        "        stats[num]['frequence_relative'] = len(indices_apparitions) / nombre_total_tirages if nombre_total_tirages > 0 else 0\n",
        "        if not indices_apparitions: stats[num].update({'dernier_tirage_idx': -1, 'ecart_actuel': nombre_total_tirages, 'ecarts_observes': [], 'ecart_moyen_specifique': float('inf') if nombre_total_tirages > 0 else 0})\n",
        "        else:\n",
        "            stats[num]['dernier_tirage_idx'] = max(indices_apparitions)\n",
        "            stats[num]['ecart_actuel'] = (nombre_total_tirages - 1) - max(indices_apparitions)\n",
        "            ecarts = [indices_apparitions[j] - indices_apparitions[j-1] for j in range(1, len(indices_apparitions))] if len(indices_apparitions) > 1 else []\n",
        "            stats[num]['ecarts_observes'] = ecarts\n",
        "            stats[num]['ecart_moyen_specifique'] = np.mean(ecarts) if ecarts else float('inf')\n",
        "\n",
        "    if 'Complementaire' in historique_df_asc.columns:\n",
        "        for num_c in range(1, max_num_comp + 1):\n",
        "            cle_stat = f\"C{num_c}\"; stats[cle_stat] = {'type':'complementaire', 'valeur_reelle': num_c}\n",
        "            try:\n",
        "                apparitions_c = (pd.to_numeric(historique_df_asc['Complementaire'], errors='coerce') == num_c)\n",
        "                indices_apparitions_c = historique_df_asc[apparitions_c].index.tolist()\n",
        "            except Exception as e: logging.error(f\"Erreur calcul apparitions N°C {num_c}: {e}\"); indices_apparitions_c = []\n",
        "\n",
        "            stats[cle_stat]['frequence_absolue'] = len(indices_apparitions_c)\n",
        "            stats[cle_stat]['frequence_relative'] = len(indices_apparitions_c) / nombre_total_tirages if nombre_total_tirages > 0 else 0\n",
        "            if not indices_apparitions_c: stats[cle_stat].update({'dernier_tirage_idx': -1, 'ecart_actuel': nombre_total_tirages, 'ecarts_observes': [], 'ecart_moyen_specifique': float('inf') if nombre_total_tirages > 0 else 0})\n",
        "            else:\n",
        "                stats[cle_stat]['dernier_tirage_idx'] = max(indices_apparitions_c)\n",
        "                stats[cle_stat]['ecart_actuel'] = (nombre_total_tirages - 1) - max(indices_apparitions_c)\n",
        "                ecarts_c = [indices_apparitions_c[j] - indices_apparitions_c[j-1] for j in range(1, len(indices_apparitions_c))] if len(indices_apparitions_c) > 1 else []\n",
        "                stats[cle_stat]['ecarts_observes'] = ecarts_c\n",
        "                stats[cle_stat]['ecart_moyen_specifique'] = np.mean(ecarts_c) if ecarts_c else float('inf')\n",
        "    else: logging.warning(\"Colonne 'Complementaire' non trouvée pour calculer_statistiques_avancees.\")\n",
        "    return stats\n",
        "\n",
        "def calculer_score_prediction_custom(num_key, num_stat_dict, nombre_total_tirages_hist, poids, moyenne_globale_ecart_ref_principaux=8.8, moyenne_globale_ecart_ref_comp=9.0):\n",
        "    score = 0.0\n",
        "    if num_key not in num_stat_dict or not num_stat_dict[num_key]:\n",
        "        logging.warning(f\"Clé {num_key} non trouvée ou stats vides dans num_stat_dict pour calculer_score_prediction_custom.\")\n",
        "        return 0.0\n",
        "\n",
        "    num_stats = num_stat_dict[num_key]\n",
        "    required_keys = ['frequence_absolue', 'frequence_relative', 'ecart_actuel', 'ecart_moyen_specifique', 'type']\n",
        "    if not all(key in num_stats for key in required_keys):\n",
        "        logging.warning(f\"Statistiques manquantes pour la clé {num_key}: {num_stats}\")\n",
        "        return 0.0\n",
        "\n",
        "    if num_stats['frequence_absolue'] > 0: score += num_stats['frequence_relative'] * poids['frequence']\n",
        "\n",
        "    ecart_actuel = num_stats['ecart_actuel']\n",
        "    ecart_moyen_specifique = num_stats['ecart_moyen_specifique']\n",
        "    moyenne_globale_ecart_ref = moyenne_globale_ecart_ref_principaux if num_stats['type'] == 'principal' else moyenne_globale_ecart_ref_comp\n",
        "\n",
        "    if ecart_moyen_specifique != float('inf') and ecart_moyen_specifique > 0:\n",
        "        proximite_specifique = 1.0 - (abs(ecart_actuel - ecart_moyen_specifique) / ecart_moyen_specifique)\n",
        "        score += max(0, proximite_specifique) * poids['proximite_ecart_moyen_spec']\n",
        "        if ecart_actuel > ecart_moyen_specifique: score += ((ecart_actuel / ecart_moyen_specifique) - 1.0) * poids['depassement_ecart_moyen_spec']\n",
        "\n",
        "    if moyenne_globale_ecart_ref > 0 :\n",
        "        proximite_globale = 1.0 - (abs(ecart_actuel - moyenne_globale_ecart_ref) / moyenne_globale_ecart_ref)\n",
        "        score += max(0, proximite_globale) * poids['proximite_ecart_global_ref']\n",
        "        if ecart_actuel > moyenne_globale_ecart_ref: score += ((ecart_actuel / moyenne_globale_ecart_ref) - 1.0) * poids['depassement_ecart_global_ref']\n",
        "\n",
        "    if num_stats['frequence_absolue'] == 0: score += poids.get('bonus_jamais_sorti', 0.1)\n",
        "    elif nombre_total_tirages_hist > 0 and num_stats['frequence_absolue'] < (nombre_total_tirages_hist * 0.01):\n",
        "        score += poids.get('bonus_rare', 0.05)\n",
        "    return score\n",
        "\n",
        "def evaluer_performance_poids(poids, historique_complet_df_asc, nombre_tirages_backtest, min_hist_req):\n",
        "    if len(historique_complet_df_asc) < (nombre_tirages_backtest + min_hist_req):\n",
        "        logging.warning(f\"Pas assez de données ({len(historique_complet_df_asc)}) pour backtest ({nombre_tirages_backtest + min_hist_req} requis).\")\n",
        "        return -float('inf')\n",
        "\n",
        "    total_bons_numeros_principaux_trouves = 0\n",
        "    index_debut_periode_test = len(historique_complet_df_asc) - nombre_tirages_backtest\n",
        "\n",
        "    if index_debut_periode_test < min_hist_req:\n",
        "        logging.warning(f\"Le premier historique pour backtest ({index_debut_periode_test} tirages) est < min_hist_req ({min_hist_req}).\")\n",
        "        return -float('inf')\n",
        "\n",
        "    for i in range(index_debut_periode_test, len(historique_complet_df_asc)):\n",
        "        donnees_pour_prediction = historique_complet_df_asc.iloc[:i]\n",
        "        tirage_reel_a_predire = historique_complet_df_asc.iloc[i]\n",
        "\n",
        "        if len(donnees_pour_prediction) < min_hist_req:\n",
        "            logging.debug(f\"Skipping backtest iteration {i}, not enough history: {len(donnees_pour_prediction)} < {min_hist_req}\")\n",
        "            continue\n",
        "\n",
        "        stats_temp_backtest = calculer_statistiques_avancees(donnees_pour_prediction.copy())\n",
        "        nombre_tirages_hist_temp = len(donnees_pour_prediction)\n",
        "        scores_principaux_backtest = []\n",
        "        for n_p in range(1, 50):\n",
        "            score_n_p = calculer_score_prediction_custom(n_p, stats_temp_backtest, nombre_tirages_hist_temp, poids)\n",
        "            scores_principaux_backtest.append({'numero': n_p, 'score': score_n_p})\n",
        "        scores_principaux_backtest.sort(key=lambda x: x['score'], reverse=True)\n",
        "        prediction_numeros_principaux = sorted([item['numero'] for item in scores_principaux_backtest[:5]])\n",
        "\n",
        "        try:\n",
        "            numeros_reels_principaux = sorted([int(tirage_reel_a_predire[f'N{k}']) for k in range(1,6)])\n",
        "        except ValueError:\n",
        "            logging.error(f\"Erreur de conversion des numéros réels en int pour le backtest à l'index {i}: {tirage_reel_a_predire}\")\n",
        "            continue\n",
        "\n",
        "        bons_numeros_ce_tirage = len(set(prediction_numeros_principaux) & set(numeros_reels_principaux))\n",
        "        total_bons_numeros_principaux_trouves += bons_numeros_ce_tirage\n",
        "    return total_bons_numeros_principaux_trouves\n",
        "\n",
        "def optimiser_poids_par_grille(historique_complet_df_asc, nombre_tirages_backtest=49, min_hist_req=56):\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n OPTIMISATION DES POIDS (RECHERCHE PAR GRILLE) \\n\" + \"=\"*40)\n",
        "    grille_valeurs_poids = {\n",
        "        'frequence': [0.1, 0.14],\n",
        "        'proximite_ecart_moyen_spec': [0.4, 0.7],\n",
        "        'depassement_ecart_moyen_spec': [0.2, 0.3],\n",
        "        'proximite_ecart_global_ref': [0.1, 0.14],\n",
        "        'depassement_ecart_global_ref': [0.02, 0.07],\n",
        "        'bonus_jamais_sorti': [0.07, 0.1],\n",
        "        'bonus_rare': [0.02, 0.07]\n",
        "    }\n",
        "    cles_des_poids = list(grille_valeurs_poids.keys())\n",
        "    valeurs_combinaisons_poids = [grille_valeurs_poids[k] for k in cles_des_poids]\n",
        "    meilleurs_poids_trouves = None\n",
        "    meilleure_performance_obtenue = -1\n",
        "\n",
        "    combinaisons_de_poids_a_tester = list(product(*valeurs_combinaisons_poids))\n",
        "    nombre_total_combinaisons = len(combinaisons_de_poids_a_tester)\n",
        "\n",
        "    if nombre_total_combinaisons > MAX_GRID_SEARCH_CUSTOM_COMBINATIONS:\n",
        "        print(f\"Attention: Limitant le test à {MAX_GRID_SEARCH_CUSTOM_COMBINATIONS} combinaisons de poids sur {nombre_total_combinaisons}.\")\n",
        "        combinaisons_de_poids_a_tester = combinaisons_de_poids_a_tester[:MAX_GRID_SEARCH_CUSTOM_COMBINATIONS]\n",
        "        nombre_total_combinaisons = MAX_GRID_SEARCH_CUSTOM_COMBINATIONS\n",
        "\n",
        "\n",
        "    print(f\"Début du test de {nombre_total_combinaisons} combinaisons de poids...\")\n",
        "    if nombre_total_combinaisons == 0:\n",
        "        print(\"Aucune combinaison de poids à tester.\"); return {'frequence':0.1, 'proximite_ecart_moyen_spec':0.5, 'depassement_ecart_moyen_spec':0.3, 'proximite_ecart_global_ref':0.1, 'depassement_ecart_global_ref':0.05, 'bonus_jamais_sorti':0.1, 'bonus_rare':0.05}\n",
        "\n",
        "    for i, valeurs_poids_actuels in enumerate(combinaisons_de_poids_a_tester):\n",
        "        poids_actuels_dict = dict(zip(cles_des_poids, valeurs_poids_actuels))\n",
        "        if (i + 1) % 10 == 0 or i == 0 or (i + 1) == nombre_total_combinaisons :\n",
        "            print(f\"  Test de la combinaison de poids {i+1}/{nombre_total_combinaisons}...\")\n",
        "\n",
        "        performance_actuelle = evaluer_performance_poids(\n",
        "            poids_actuels_dict, historique_complet_df_asc,\n",
        "            nombre_tirages_backtest, min_hist_req\n",
        "        )\n",
        "        if performance_actuelle > meilleure_performance_obtenue:\n",
        "            meilleure_performance_obtenue = performance_actuelle\n",
        "            meilleurs_poids_trouves = poids_actuels_dict\n",
        "            print(f\"    NOUVEAUX MEILLEURS POIDS TROUVÉS ! Combinaison {i+1}. Perf: {meilleure_performance_obtenue}.\")\n",
        "            if meilleurs_poids_trouves:\n",
        "                try:\n",
        "                    with open(\"meilleurs_poids_intermediaires.json\", \"w\") as f_json:\n",
        "                        json.dump({'poids': meilleurs_poids_trouves, 'performance': float(meilleure_performance_obtenue)}, f_json)\n",
        "                except Exception as e_save: print(f\"Erreur sauvegarde intermédiaire: {e_save}\")\n",
        "\n",
        "    if meilleurs_poids_trouves:\n",
        "        print(f\"Meilleurs poids trouvés : {meilleurs_poids_trouves}\")\n",
        "        print(f\"Meilleure performance : {meilleure_performance_obtenue}\")\n",
        "    else:\n",
        "        print(\"Aucune combinaison de poids n'a amélioré la performance. Poids par défaut.\")\n",
        "        meilleurs_poids_trouves = {'frequence':0.1, 'proximite_ecart_moyen_spec':0.5, 'depassement_ecart_moyen_spec':0.3, 'proximite_ecart_global_ref':0.1, 'depassement_ecart_global_ref':0.05, 'bonus_jamais_sorti':0.1, 'bonus_rare':0.05}\n",
        "    print(\"=\"*60)\n",
        "    return meilleurs_poids_trouves\n",
        "\n",
        "def preparer_donnees_pour_ml_numero(historique_df_asc, num_cible, type_cible='principal'):\n",
        "    X_data, y_data = [], []\n",
        "    min_historique_pour_features = 21\n",
        "    if len(historique_df_asc) < min_historique_pour_features:\n",
        "        logging.warning(f\"Historique trop court ({len(historique_df_asc)}) pour préparer les données ML pour {num_cible}.\")\n",
        "        return np.array(X_data), np.array(y_data)\n",
        "\n",
        "    for i in range(min_historique_pour_features, len(historique_df_asc)):\n",
        "        hist_avant_tirage_i_dans_fenetre = historique_df_asc.iloc[:i]\n",
        "        tirage_actuel_i = historique_df_asc.iloc[i]\n",
        "\n",
        "        stats_avant_i = calculer_statistiques_avancees(hist_avant_tirage_i_dans_fenetre.copy())\n",
        "        cle_stat_cible = num_cible if type_cible == 'principal' else f\"C{num_cible}\"\n",
        "\n",
        "        if cle_stat_cible not in stats_avant_i or not stats_avant_i[cle_stat_cible]:\n",
        "            logging.debug(f\"Clé {cle_stat_cible} non trouvée ou stats vides dans stats_avant_i pour tirage {i} (index df) dans la préparation ML.\")\n",
        "            continue\n",
        "        stat_num_cible = stats_avant_i[cle_stat_cible]\n",
        "        sortie_tirage_precedent, apparitions_3_derniers, apparitions_5_derniers = 0, 0, 0\n",
        "\n",
        "        if not hist_avant_tirage_i_dans_fenetre.empty:\n",
        "            dernier_tirage_dans_hist = hist_avant_tirage_i_dans_fenetre.iloc[-1]\n",
        "            try:\n",
        "                if type_cible == 'principal':\n",
        "                    if num_cible in {int(dernier_tirage_dans_hist[f'N{k}']) for k in range(1,6) if pd.notna(dernier_tirage_dans_hist[f'N{k}'])}:\n",
        "                        sortie_tirage_precedent = 1\n",
        "                else:\n",
        "                    if pd.notna(dernier_tirage_dans_hist['Complementaire']) and int(dernier_tirage_dans_hist['Complementaire']) == num_cible:\n",
        "                        sortie_tirage_precedent = 1\n",
        "            except ValueError: logging.warning(f\"Erreur conversion int 'sortie_tirage_precedent' (ML) N°{num_cible}.\")\n",
        "\n",
        "            hist_3_derniers = hist_avant_tirage_i_dans_fenetre.tail(3)\n",
        "            for _, row_3 in hist_3_derniers.iterrows():\n",
        "                try:\n",
        "                    if type_cible == 'principal':\n",
        "                        if num_cible in {int(row_3[f'N{k}']) for k in range(1,6) if pd.notna(row_3[f'N{k}'])}: apparitions_3_derniers += 1\n",
        "                    else:\n",
        "                        if pd.notna(row_3['Complementaire']) and int(row_3['Complementaire']) == num_cible: apparitions_3_derniers += 1\n",
        "                except ValueError: logging.warning(f\"Erreur conversion int 'apparitions_3_derniers' (ML) N°{num_cible}.\")\n",
        "\n",
        "            hist_5_derniers = hist_avant_tirage_i_dans_fenetre.tail(5)\n",
        "            for _, row_5 in hist_5_derniers.iterrows():\n",
        "                try:\n",
        "                    if type_cible == 'principal':\n",
        "                        if num_cible in {int(row_5[f'N{k}']) for k in range(1,6) if pd.notna(row_5[f'N{k}'])}: apparitions_5_derniers += 1\n",
        "                    else:\n",
        "                        if pd.notna(row_5['Complementaire']) and int(row_5['Complementaire']) == num_cible: apparitions_5_derniers += 1\n",
        "                except ValueError: logging.warning(f\"Erreur conversion int 'apparitions_5_derniers' (ML) N°{num_cible}.\")\n",
        "\n",
        "        ecart_moyen_spec_feature = stat_num_cible['ecart_moyen_specifique']\n",
        "        if ecart_moyen_spec_feature == float('inf'):\n",
        "            ecart_moyen_spec_feature = len(hist_avant_tirage_i_dans_fenetre) if stat_num_cible['frequence_absolue'] <=1 else -1\n",
        "\n",
        "        features_pour_exemple = [\n",
        "            stat_num_cible.get('frequence_relative', 0.0),\n",
        "            stat_num_cible.get('ecart_actuel', len(hist_avant_tirage_i_dans_fenetre)),\n",
        "            ecart_moyen_spec_feature,\n",
        "            len(stat_num_cible.get('ecarts_observes', [])),\n",
        "            sortie_tirage_precedent, apparitions_3_derniers, apparitions_5_derniers\n",
        "        ]\n",
        "        current_X_len = len(X_data)\n",
        "        X_data.append(features_pour_exemple)\n",
        "\n",
        "        try:\n",
        "            if type_cible == 'principal':\n",
        "                numeros_gagnants_principaux_i = {int(tirage_actuel_i[f'N{k}']) for k in range(1,6) if pd.notna(tirage_actuel_i[f'N{k}'])}\n",
        "                y_data.append(1 if num_cible in numeros_gagnants_principaux_i else 0)\n",
        "            else:\n",
        "                if pd.notna(tirage_actuel_i['Complementaire']):\n",
        "                    y_data.append(1 if int(tirage_actuel_i['Complementaire']) == num_cible else 0)\n",
        "                else: y_data.append(0)\n",
        "        except (ValueError, TypeError) as e_target:\n",
        "            logging.error(f\"Erreur conversion cible y N°{num_cible} tirage index {i}: {e_target}. Ligne: {tirage_actuel_i}\")\n",
        "            if len(X_data) > current_X_len : X_data.pop()\n",
        "            continue\n",
        "    return np.array(X_data), np.array(y_data)\n",
        "\n",
        "def _calculer_features_recence_pour_prediction(donnees_historiques_fenetre, num_cible_pred, type_cible_pred):\n",
        "    sortie_tirage_precedent_pred, apparitions_3_derniers_pred, apparitions_5_derniers_pred = 0, 0, 0\n",
        "    if not donnees_historiques_fenetre.empty:\n",
        "        dernier_tirage_hist_pred = donnees_historiques_fenetre.iloc[-1]\n",
        "        try:\n",
        "            if type_cible_pred == 'principal':\n",
        "                if all(f'N{k}' in dernier_tirage_hist_pred for k in range(1,6)):\n",
        "                    if num_cible_pred in {int(dernier_tirage_hist_pred[f'N{k}']) for k in range(1,6) if pd.notna(dernier_tirage_hist_pred[f'N{k}'])}:\n",
        "                        sortie_tirage_precedent_pred = 1\n",
        "            else:\n",
        "                if 'Complementaire' in dernier_tirage_hist_pred and pd.notna(dernier_tirage_hist_pred['Complementaire']):\n",
        "                    if int(dernier_tirage_hist_pred['Complementaire']) == num_cible_pred: sortie_tirage_precedent_pred = 1\n",
        "        except (ValueError, TypeError) as e_pred_rec: logging.warning(f\"Erreur conversion _calculer_features_recence (dernier) N°{num_cible_pred}: {e_pred_rec}\")\n",
        "\n",
        "        hist_3_derniers_pred = donnees_historiques_fenetre.tail(3)\n",
        "        for _, row_3_pred in hist_3_derniers_pred.iterrows():\n",
        "            try:\n",
        "                if type_cible_pred == 'principal':\n",
        "                    if all(f'N{k}' in row_3_pred for k in range(1,6)):\n",
        "                        if num_cible_pred in {int(row_3_pred[f'N{k}']) for k in range(1,6) if pd.notna(row_3_pred[f'N{k}'])}: apparitions_3_derniers_pred += 1\n",
        "                else:\n",
        "                    if 'Complementaire' in row_3_pred and pd.notna(row_3_pred['Complementaire']):\n",
        "                        if int(row_3_pred['Complementaire']) == num_cible_pred: apparitions_3_derniers_pred += 1\n",
        "            except (ValueError, TypeError) as e_pred_rec3: logging.warning(f\"Erreur conversion _calculer_features_recence (3 derniers) N°{num_cible_pred}: {e_pred_rec3}\")\n",
        "\n",
        "        hist_5_derniers_pred = donnees_historiques_fenetre.tail(5)\n",
        "        for _, row_5_pred in hist_5_derniers_pred.iterrows():\n",
        "            try:\n",
        "                if type_cible_pred == 'principal':\n",
        "                    if all(f'N{k}' in row_5_pred for k in range(1,6)):\n",
        "                        if num_cible_pred in {int(row_5_pred[f'N{k}']) for k in range(1,6) if pd.notna(row_5_pred[f'N{k}'])}: apparitions_5_derniers_pred += 1\n",
        "                else:\n",
        "                    if pd.notna(row_5_pred['Complementaire']) and int(row_5_pred['Complementaire']) == num_cible_pred: apparitions_5_derniers_pred += 1\n",
        "            except (ValueError, TypeError) as e_pred_rec5: logging.warning(f\"Erreur conversion _calculer_features_recence (5 derniers) N°{num_cible_pred}: {e_pred_rec5}\")\n",
        "    return sortie_tirage_precedent_pred, apparitions_3_derniers_pred, apparitions_5_derniers_pred\n",
        "\n",
        "def entrainer_et_predire_ml_pour_backtest(historique_complet_asc_total, index_tirage_a_predire, modeles_principaux, modeles_complementaires, taille_fenetre_entrainement_ml=98):\n",
        "    debut_fenetre = max(0, index_tirage_a_predire - taille_fenetre_entrainement_ml)\n",
        "    donnees_entrainement_ml_fenetre = historique_complet_asc_total.iloc[debut_fenetre:index_tirage_a_predire]\n",
        "    min_hist_pour_features_ml = 21\n",
        "\n",
        "    if len(donnees_entrainement_ml_fenetre) < min_hist_pour_features_ml :\n",
        "        logging.warning(f\"Fenêtre d'entraînement ML trop petite ({len(donnees_entrainement_ml_fenetre)}, min {min_hist_pour_features_ml}) pour tirage index {index_tirage_a_predire}. Prédictions ML par défaut.\")\n",
        "        pred_p_ml_default = sorted(list(MES_NUMEROS_PRINCIPAUX_JOUES))[:5] if MES_NUMEROS_PRINCIPAUX_JOUES else random.sample(range(1,50),5)\n",
        "        pred_c_ml_default = list(MES_COMPLEMENTAIRES_POSSIBLES_JOUES)[0] if MES_COMPLEMENTAIRES_POSSIBLES_JOUES else random.randint(1,10)\n",
        "        return pred_p_ml_default, pred_c_ml_default\n",
        "\n",
        "    print(f\"      Utilisation d'une fenêtre de {len(donnees_entrainement_ml_fenetre)} tirages (indices {debut_fenetre} à {index_tirage_a_predire-1}) pour l'entraînement ML du tirage à l'index {index_tirage_a_predire}.\")\n",
        "\n",
        "    stats_pour_creation_features_prediction = calculer_statistiques_avancees(donnees_entrainement_ml_fenetre.copy())\n",
        "\n",
        "    param_grid_rf = {'n_estimators': [50, 100], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 3, 5]}\n",
        "    default_n_estimators = 100\n",
        "\n",
        "    predictions_probabilites_principaux = []\n",
        "    for num_p in range(1, 50):\n",
        "        X_train_num_p, y_train_num_p = preparer_donnees_pour_ml_numero(donnees_entrainement_ml_fenetre.copy(), num_p, 'principal')\n",
        "\n",
        "        if len(X_train_num_p) < 10 or len(np.unique(y_train_num_p)) < 2:\n",
        "            modeles_principaux[num_p] = None\n",
        "            predictions_probabilites_principaux.append({'numero': num_p, 'proba': 0.0})\n",
        "            continue\n",
        "\n",
        "        current_model_p = None\n",
        "        if USE_GRID_SEARCH_FOR_FULL_ANALYSIS_ML:\n",
        "            cv_splits = min(3, len(X_train_num_p) // (sum(param_grid_rf.get('min_samples_split', [2])) // len(param_grid_rf.get('min_samples_split', [2]))) if len(X_train_num_p) > 20 else 2)\n",
        "            cv_splits = max(2, cv_splits)\n",
        "            grid_search_p = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), param_grid_rf, cv=cv_splits, scoring='roc_auc', n_jobs=-1, error_score='raise')\n",
        "            try:\n",
        "                grid_search_p.fit(X_train_num_p, y_train_num_p)\n",
        "                current_model_p = grid_search_p.best_estimator_\n",
        "            except ValueError as e_gs_p:\n",
        "                logging.warning(f\"Erreur GridSearchCV Principal N°{num_p} (Backtest): {e_gs_p}. Modèle par défaut.\")\n",
        "                current_model_p = RandomForestClassifier(n_estimators=default_n_estimators, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "                current_model_p.fit(X_train_num_p, y_train_num_p)\n",
        "        else:\n",
        "            current_model_p = RandomForestClassifier(n_estimators=default_n_estimators, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "            current_model_p.fit(X_train_num_p, y_train_num_p)\n",
        "        modeles_principaux[num_p] = current_model_p\n",
        "\n",
        "        if num_p not in stats_pour_creation_features_prediction:\n",
        "            logging.warning(f\"Stats non trouvées N°P {num_p} création features prédiction (backtest).\")\n",
        "            predictions_probabilites_principaux.append({'numero': num_p, 'proba': 0.0})\n",
        "            continue\n",
        "        stat_num_cible_p = stats_pour_creation_features_prediction[num_p]\n",
        "        s_t_p_pred, a_3_d_pred, a_5_d_pred = _calculer_features_recence_pour_prediction(donnees_entrainement_ml_fenetre, num_p, 'principal')\n",
        "        ecart_moyen_spec_feature_pred_p = stat_num_cible_p.get('ecart_moyen_specifique', float('inf'))\n",
        "        if ecart_moyen_spec_feature_pred_p == float('inf'):\n",
        "            ecart_moyen_spec_feature_pred_p = len(donnees_entrainement_ml_fenetre) if stat_num_cible_p.get('frequence_absolue',0) <=1 else -1\n",
        "        features_pour_prediction_p = np.array([[\n",
        "            stat_num_cible_p.get('frequence_relative', 0.0),\n",
        "            stat_num_cible_p.get('ecart_actuel', len(donnees_entrainement_ml_fenetre)),\n",
        "            ecart_moyen_spec_feature_pred_p,\n",
        "            len(stat_num_cible_p.get('ecarts_observes', [])),\n",
        "            s_t_p_pred, a_3_d_pred, a_5_d_pred\n",
        "        ]])\n",
        "        if modeles_principaux[num_p]:\n",
        "            try:\n",
        "                proba_sortie_p = modeles_principaux[num_p].predict_proba(features_pour_prediction_p)[0][1]\n",
        "                predictions_probabilites_principaux.append({'numero': num_p, 'proba': proba_sortie_p})\n",
        "            except Exception as e_predict_p:\n",
        "                logging.error(f\"Erreur predict_proba Principal N°{num_p} (Backtest): {e_predict_p}\")\n",
        "                predictions_probabilites_principaux.append({'numero': num_p, 'proba': 0.0})\n",
        "        else: predictions_probabilites_principaux.append({'numero': num_p, 'proba': 0.0})\n",
        "\n",
        "\n",
        "    predictions_probabilites_complementaires = []\n",
        "    for num_c in range(1, 11):\n",
        "        X_train_num_c, y_train_num_c = preparer_donnees_pour_ml_numero(donnees_entrainement_ml_fenetre.copy(), num_c, 'complementaire')\n",
        "        cle_stat_c_pred = f\"C{num_c}\"\n",
        "\n",
        "        if len(X_train_num_c) < 10 or len(np.unique(y_train_num_c)) < 2:\n",
        "            modeles_complementaires[num_c] = None\n",
        "            predictions_probabilites_complementaires.append({'numero': num_c, 'proba': 0.0})\n",
        "            continue\n",
        "\n",
        "        current_model_c = None\n",
        "        if USE_GRID_SEARCH_FOR_FULL_ANALYSIS_ML:\n",
        "            cv_splits_c = min(3, len(X_train_num_c) // (sum(param_grid_rf.get('min_samples_split', [2])) // len(param_grid_rf.get('min_samples_split', [2]))) if len(X_train_num_c) > 20 else 2)\n",
        "            cv_splits_c = max(2, cv_splits_c)\n",
        "            grid_search_c = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), param_grid_rf, cv=cv_splits_c, scoring='roc_auc', n_jobs=-1, error_score='raise')\n",
        "            try:\n",
        "                grid_search_c.fit(X_train_num_c, y_train_num_c)\n",
        "                current_model_c = grid_search_c.best_estimator_\n",
        "            except ValueError as e_gs_c:\n",
        "                logging.warning(f\"Erreur GridSearchCV Complémentaire N°{num_c} (Backtest): {e_gs_c}. Modèle par défaut.\")\n",
        "                current_model_c = RandomForestClassifier(n_estimators=default_n_estimators, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "                current_model_c.fit(X_train_num_c, y_train_num_c)\n",
        "        else:\n",
        "            current_model_c = RandomForestClassifier(n_estimators=default_n_estimators, random_state=42, class_weight='balanced', n_jobs=-1)\n",
        "            current_model_c.fit(X_train_num_c, y_train_num_c)\n",
        "        modeles_complementaires[num_c] = current_model_c\n",
        "\n",
        "        if cle_stat_c_pred not in stats_pour_creation_features_prediction:\n",
        "            logging.warning(f\"Stats non trouvées N°C {num_c} création features prédiction (backtest).\")\n",
        "            predictions_probabilites_complementaires.append({'numero': num_c, 'proba': 0.0})\n",
        "            continue\n",
        "        stat_num_cible_c = stats_pour_creation_features_prediction[cle_stat_c_pred]\n",
        "        s_t_p_pred_c, a_3_d_pred_c, a_5_d_pred_c = _calculer_features_recence_pour_prediction(donnees_entrainement_ml_fenetre, num_c, 'complementaire')\n",
        "        ecart_moyen_spec_feature_pred_c = stat_num_cible_c.get('ecart_moyen_specifique', float('inf'))\n",
        "        if ecart_moyen_spec_feature_pred_c == float('inf'):\n",
        "            ecart_moyen_spec_feature_pred_c = len(donnees_entrainement_ml_fenetre) if stat_num_cible_c.get('frequence_absolue',0) <=1 else -1\n",
        "        features_pour_prediction_c = np.array([[\n",
        "            stat_num_cible_c.get('frequence_relative', 0.0),\n",
        "            stat_num_cible_c.get('ecart_actuel', len(donnees_entrainement_ml_fenetre)),\n",
        "            ecart_moyen_spec_feature_pred_c,\n",
        "            len(stat_num_cible_c.get('ecarts_observes', [])),\n",
        "            s_t_p_pred_c, a_3_d_pred_c, a_5_d_pred_c\n",
        "        ]])\n",
        "        if modeles_complementaires[num_c]:\n",
        "            try:\n",
        "                proba_sortie_c = modeles_complementaires[num_c].predict_proba(features_pour_prediction_c)[0][1]\n",
        "                predictions_probabilites_complementaires.append({'numero': num_c, 'proba': proba_sortie_c})\n",
        "            except Exception as e_predict_c:\n",
        "                logging.error(f\"Erreur predict_proba Complémentaire N°{num_c} (Backtest): {e_predict_c}\")\n",
        "                predictions_probabilites_complementaires.append({'numero': num_c, 'proba': 0.0})\n",
        "        else: predictions_probabilites_complementaires.append({'numero': num_c, 'proba': 0.0})\n",
        "\n",
        "    predictions_probabilites_principaux.sort(key=lambda x: x['proba'], reverse=True)\n",
        "    predictions_probabilites_complementaires.sort(key=lambda x: x['proba'], reverse=True)\n",
        "\n",
        "    prediction_principaux_ml = sorted([item['numero'] for item in predictions_probabilites_principaux[:5]])\n",
        "    prediction_complementaire_ml = predictions_probabilites_complementaires[0]['numero'] if predictions_probabilites_complementaires and predictions_probabilites_complementaires[0]['proba'] > 0 else None\n",
        "\n",
        "    return prediction_principaux_ml, prediction_complementaire_ml\n",
        "\n",
        "# --- Nouvelle fonction pour calculer les \"quasiment-gagnants\" personnalisés ---\n",
        "def calculate_custom_almost_wins(predicted_nums, actual_nums, predicted_comp=None, actual_comp=None, max_num_main=49, max_num_comp=10):\n",
        "    \"\"\"\n",
        "    Calcule les métriques de \"quasiment-gagnants\" basées sur la dizaine, la proximité, la parité, miroir et circulaire.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'num_in_same_decade': 0, 'num_plus_minus_1': 0, 'num_plus_minus_2': 0,\n",
        "        'num_same_decade_and_parity': 0, 'num_mirror': 0,\n",
        "        'num_circular_plus_minus_1': 0, 'num_circular_plus_minus_2': 0,\n",
        "        'comp_in_same_decade': 0, 'comp_plus_minus_1': 0, 'comp_plus_minus_2': 0,\n",
        "        'comp_same_decade_and_parity': 0, 'comp_mirror': 0,\n",
        "        'comp_circular_plus_minus_1': 0, 'comp_circular_plus_minus_2': 0\n",
        "    }\n",
        "\n",
        "    # --- Helper function for mirror ---\n",
        "    def get_mirror(n):\n",
        "        if 10 <= n <= 99: # S'applique aux nombres à deux chiffres\n",
        "            s = str(n)\n",
        "            if s[0] == s[1]: return None # Miroir non défini pour 11, 22 etc.\n",
        "            mirrored = int(s[1] + s[0])\n",
        "            return mirrored\n",
        "        return None\n",
        "\n",
        "    # --- Helper function for circular distance ---\n",
        "    def circular_distance(n1, n2, max_val):\n",
        "        diff = abs(n1 - n2)\n",
        "        return min(diff, max_val - diff)\n",
        "\n",
        "    # --- Analyse pour les numéros principaux ---\n",
        "    # Utiliser des sets pour compter les numéros réels uniques qui correspondent aux critères\n",
        "    # pour éviter de compter plusieurs fois le même numéro réel s'il correspond à plusieurs numéros prédits.\n",
        "    unique_matches = {key: set() for key in [\n",
        "        'num_in_same_decade', 'num_plus_minus_1', 'num_plus_minus_2_strict',\n",
        "        'num_same_decade_and_parity', 'num_mirror',\n",
        "        'num_circular_plus_minus_1', 'num_circular_plus_minus_2_strict'\n",
        "    ]}\n",
        "\n",
        "    for p_num in predicted_nums:\n",
        "        p_decade_start = (p_num - 1) // 10 * 10\n",
        "        p_is_even = (p_num % 2 == 0)\n",
        "        p_mirror = get_mirror(p_num)\n",
        "\n",
        "        for a_num in actual_nums:\n",
        "            a_decade_start = (a_num - 1) // 10 * 10\n",
        "            a_is_even = (a_num % 2 == 0)\n",
        "\n",
        "            if p_decade_start == a_decade_start:\n",
        "                unique_matches['num_in_same_decade'].add(a_num)\n",
        "                if p_is_even == a_is_even:\n",
        "                    unique_matches['num_same_decade_and_parity'].add(a_num)\n",
        "\n",
        "            diff_abs = abs(p_num - a_num)\n",
        "            if diff_abs == 1:\n",
        "                unique_matches['num_plus_minus_1'].add(a_num)\n",
        "            elif diff_abs == 2:\n",
        "                unique_matches['num_plus_minus_2_strict'].add(a_num) # Sera filtré plus tard\n",
        "\n",
        "            if p_mirror is not None and p_mirror == a_num:\n",
        "                unique_matches['num_mirror'].add(a_num)\n",
        "\n",
        "            circ_dist = circular_distance(p_num, a_num, max_num_main)\n",
        "            if circ_dist == 1:\n",
        "                unique_matches['num_circular_plus_minus_1'].add(a_num)\n",
        "            elif circ_dist == 2:\n",
        "                unique_matches['num_circular_plus_minus_2_strict'].add(a_num) # Sera filtré\n",
        "\n",
        "    results['num_in_same_decade'] = len(unique_matches['num_in_same_decade'])\n",
        "    results['num_plus_minus_1'] = len(unique_matches['num_plus_minus_1'])\n",
        "    # S'assurer que +/-2 n'inclut pas ceux déjà comptés par +/-1\n",
        "    results['num_plus_minus_2'] = len(unique_matches['num_plus_minus_2_strict'] - unique_matches['num_plus_minus_1'])\n",
        "    results['num_same_decade_and_parity'] = len(unique_matches['num_same_decade_and_parity'])\n",
        "    results['num_mirror'] = len(unique_matches['num_mirror'])\n",
        "    results['num_circular_plus_minus_1'] = len(unique_matches['num_circular_plus_minus_1'])\n",
        "    results['num_circular_plus_minus_2'] = len(unique_matches['num_circular_plus_minus_2_strict'] - unique_matches['num_circular_plus_minus_1'])\n",
        "\n",
        "\n",
        "    # --- Analyse pour le numéro complémentaire (si fourni) ---\n",
        "    if predicted_comp is not None and actual_comp is not None:\n",
        "        p_comp_decade_start = (predicted_comp - 1) // 10 * 10\n",
        "        p_comp_is_even = (predicted_comp % 2 == 0)\n",
        "        p_comp_mirror = get_mirror(predicted_comp)\n",
        "\n",
        "        a_comp_decade_start = (actual_comp - 1) // 10 * 10\n",
        "        a_comp_is_even = (actual_comp % 2 == 0)\n",
        "\n",
        "        if p_comp_decade_start == a_comp_decade_start:\n",
        "            results['comp_in_same_decade'] = 1\n",
        "            if p_comp_is_even == a_comp_is_even:\n",
        "                results['comp_same_decade_and_parity'] = 1\n",
        "\n",
        "        diff_abs_comp = abs(predicted_comp - actual_comp)\n",
        "        if diff_abs_comp == 1:\n",
        "            results['comp_plus_minus_1'] = 1\n",
        "        elif diff_abs_comp == 2: # Ne sera compté que si ce n'est pas déjà +/-1\n",
        "            results['comp_plus_minus_2'] = 1\n",
        "\n",
        "        if p_comp_mirror is not None and p_comp_mirror == actual_comp:\n",
        "            results['comp_mirror'] = 1\n",
        "\n",
        "        circ_dist_comp = circular_distance(predicted_comp, actual_comp, max_num_comp)\n",
        "        if circ_dist_comp == 1:\n",
        "            results['comp_circular_plus_minus_1'] = 1\n",
        "        elif circ_dist_comp == 2: # Ne sera compté que si ce n'est pas déjà circ +/-1\n",
        "             results['comp_circular_plus_minus_2'] = 1\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def tester_performance_predictive_historique(historique_complet_df_asc, nombre_de_tirages_a_tester, poids_optimises):\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n BACKTESTING COMPARATIF (Modèle Custom vs Modèle ML) \\n\" + \"=\"*40)\n",
        "    min_historique_requis_backtest_global = 63\n",
        "    taille_fenetre_entrainement_ml_backtest = 105\n",
        "\n",
        "    if len(historique_complet_df_asc) < (max(min_historique_requis_backtest_global, taille_fenetre_entrainement_ml_backtest) + nombre_de_tirages_a_tester):\n",
        "        print(f\"Pas assez de données pour backtest complet ({len(historique_complet_df_asc)}).\")\n",
        "        print(f\"Besoin d'au moins {max(min_historique_requis_backtest_global, taille_fenetre_entrainement_ml_backtest) + nombre_de_tirages_a_tester} tirages.\")\n",
        "        return [], [] # Retourner des listes vides pour les résultats\n",
        "\n",
        "    # Initialisation des listes pour stocker les résultats détaillés par tirage\n",
        "    backtest_details_custom = []\n",
        "    backtest_details_ml = []\n",
        "\n",
        "    modeles_ml_principaux_backtest = {num: None for num in range(1,50)}\n",
        "    modeles_ml_complementaires_backtest = {num: None for num in range(1,11)}\n",
        "    index_debut_backtest_loop = len(historique_complet_df_asc) - nombre_de_tirages_a_tester\n",
        "\n",
        "    for i in range(index_debut_backtest_loop, len(historique_complet_df_asc)):\n",
        "        donnees_historiques_pour_prediction_i = historique_complet_df_asc.iloc[:i]\n",
        "        tirage_reel_i = historique_complet_df_asc.iloc[i]\n",
        "\n",
        "        if len(donnees_historiques_pour_prediction_i) < min_historique_requis_backtest_global:\n",
        "            logging.info(f\"Skipping backtest tirage index {i}: hist custom insuffisant ({len(donnees_historiques_pour_prediction_i)}).\")\n",
        "            continue\n",
        "\n",
        "        # --- Modèle Custom ---\n",
        "        stats_custom_i = calculer_statistiques_avancees(donnees_historiques_pour_prediction_i.copy())\n",
        "        nb_tirages_hist_i = len(donnees_historiques_pour_prediction_i)\n",
        "        scores_p_custom_i = [{'numero':n, 'score':calculer_score_prediction_custom(n, stats_custom_i, nb_tirages_hist_i, poids_optimises)} for n in range(1,50)]\n",
        "        scores_c_custom_i = [{'numero':n, 'score':calculer_score_prediction_custom(f\"C{n}\", stats_custom_i, nb_tirages_hist_i, poids_optimises)} for n in range(1,11)]\n",
        "        scores_p_custom_i.sort(key=lambda x: x['score'], reverse=True)\n",
        "        scores_c_custom_i.sort(key=lambda x: x['score'], reverse=True)\n",
        "        pred_p_custom_i = sorted([item['numero'] for item in scores_p_custom_i[:5]])\n",
        "        pred_c_custom_i = scores_c_custom_i[0]['numero'] if scores_c_custom_i and scores_c_custom_i[0]['score'] > 0 else None\n",
        "\n",
        "        # --- Modèle ML ---\n",
        "        pred_p_ml_i, pred_c_ml_i = [\"N/A\"]*5, \"N/A\" # Default\n",
        "        if RUN_BACKTESTING_ML_IN_FULL_ANALYSIS:\n",
        "            if len(donnees_historiques_pour_prediction_i) >= taille_fenetre_entrainement_ml_backtest:\n",
        "                print(\"  Début entraînement/prédiction ML pour ce tirage du backtest...\")\n",
        "                pred_p_ml_i, pred_c_ml_i = entrainer_et_predire_ml_pour_backtest(\n",
        "                    historique_complet_df_asc, i, modeles_ml_principaux_backtest,\n",
        "                    modeles_ml_complementaires_backtest, taille_fenetre_entrainement_ml=taille_fenetre_entrainement_ml_backtest\n",
        "                )\n",
        "                print(\"  Fin entraînement/prédiction ML pour ce tirage du backtest.\")\n",
        "            else:\n",
        "                print(f\"  Skipping ML pour tirage index {i}: hist ML insuffisant ({len(donnees_historiques_pour_prediction_i)} < {taille_fenetre_entrainement_ml_backtest}).\")\n",
        "\n",
        "        # --- Numéros Réels ---\n",
        "        try:\n",
        "            reels_p_i = sorted([int(tirage_reel_i[f'N{k}']) for k in range(1,6)])\n",
        "            reel_c_i = int(tirage_reel_i['Complementaire']) if pd.notna(tirage_reel_i['Complementaire']) else None\n",
        "        except (ValueError, TypeError) as e_reel:\n",
        "            logging.error(f\"Erreur conversion numéros réels backtest index {i}: {tirage_reel_i}, {e_reel}\")\n",
        "            continue\n",
        "\n",
        "        # --- Évaluation Modèle Custom ---\n",
        "        bons_p_custom_i = len(set(pred_p_custom_i) & set(reels_p_i))\n",
        "        bon_c_custom_i = 1 if pred_c_custom_i is not None and reel_c_i is not None and pred_c_custom_i == reel_c_i else 0\n",
        "        aw_custom_results_i = calculate_custom_almost_wins(pred_p_custom_i, reels_p_i, pred_c_custom_i, reel_c_i)\n",
        "\n",
        "        current_custom_detail = {\n",
        "            'date': tirage_reel_i['Date'], 'bons_p': bons_p_custom_i, 'bon_c': bon_c_custom_i,\n",
        "            **{f\"aw_{k}\": v for k, v in aw_custom_results_i.items()} # Ajoute tous les \"almost wins\"\n",
        "        }\n",
        "        backtest_details_custom.append(current_custom_detail)\n",
        "\n",
        "        # --- Évaluation Modèle ML ---\n",
        "        bons_p_ml_i_val = 0\n",
        "        bon_c_ml_i_val = 0\n",
        "        aw_ml_results_i = {} # Initialiser vide\n",
        "        if RUN_BACKTESTING_ML_IN_FULL_ANALYSIS and pred_p_ml_i != [\"N/A\"]*5 : # S'assurer que la prédiction ML a eu lieu\n",
        "            bons_p_ml_i_val = len(set(pred_p_ml_i) & set(reels_p_i))\n",
        "            bon_c_ml_i_val = 1 if pred_c_ml_i is not None and pred_c_ml_i != \"N/A\" and reel_c_i is not None and pred_c_ml_i == reel_c_i else 0\n",
        "            aw_ml_results_i = calculate_custom_almost_wins(pred_p_ml_i, reels_p_i, pred_c_ml_i, reel_c_i)\n",
        "\n",
        "        current_ml_detail = {\n",
        "            'date': tirage_reel_i['Date'], 'bons_p': bons_p_ml_i_val, 'bon_c': bon_c_ml_i_val,\n",
        "            **{f\"aw_{k}\": v for k, v in aw_ml_results_i.items()}\n",
        "        }\n",
        "        backtest_details_ml.append(current_ml_detail)\n",
        "\n",
        "\n",
        "        print(f\"  Résultats pour tirage {tirage_reel_i['Date'].strftime('%Y-%m-%d')}:\")\n",
        "        print(f\"    Modèle Custom: Prédit P:{pred_p_custom_i} + C:{pred_c_custom_i} -> Trouvé {bons_p_custom_i}/5 N°P, {bon_c_custom_i}/1 N°C\")\n",
        "        if RUN_BACKTESTING_ML_IN_FULL_ANALYSIS:\n",
        "             print(f\"    Modèle ML (RF): Prédit P:{pred_p_ml_i} + C:{pred_c_ml_i} -> Trouvé {bons_p_ml_i_val}/5 N°P, {bon_c_ml_i_val}/1 N°C\")\n",
        "        print(f\"    Tirage Réel         : P:{reels_p_i} + C:{reel_c_i}\")\n",
        "\n",
        "    # --- Affichage des résumés de backtest ---\n",
        "    df_backtest_custom = pd.DataFrame(backtest_details_custom)\n",
        "    df_backtest_ml = pd.DataFrame(backtest_details_ml)\n",
        "\n",
        "    for nom_modele, df_results in [(\"Modèle Custom (Poids Optimisés)\", df_backtest_custom),\n",
        "                                   (\"Modèle ML (Random Forest)\", df_backtest_ml)]:\n",
        "        if not RUN_BACKTESTING_ML_IN_FULL_ANALYSIS and nom_modele == \"Modèle ML (Random Forest)\":\n",
        "            continue\n",
        "        if not df_results.empty:\n",
        "            print(f\"\\n--- Résumé Détaillé du Backtest pour {nom_modele} sur {len(df_results)} tirages ---\")\n",
        "            print(f\"  Nombre total de bons numéros principaux trouvés : {df_results['bons_p'].sum()}\")\n",
        "            print(f\"  Nombre total de bons numéros complémentaires trouvés : {df_results['bon_c'].sum()}\")\n",
        "            print(f\"  Meilleure prédiction de numéros principaux en un tirage : {df_results['bons_p'].max()}/5\")\n",
        "            for k_bons in range(6):\n",
        "                count_k = (df_results['bons_p'] == k_bons).sum()\n",
        "                if count_k > 0:\n",
        "                    print(f\"    Nombre de fois où {k_bons} bons N°P ont été trouvés : {count_k}\")\n",
        "\n",
        "            if RUN_ALMOST_WINNERS_ANALYSIS_IN_BACKTEST:\n",
        "                print(f\"  'Quasiment-Gagnants' (Rangs Loto classiques):\")\n",
        "                # Recalculer les rangs Loto classiques si nécessaire ou les stocker pendant la boucle\n",
        "                aw_loto_4_sur_5 = (df_results['bons_p'] == 4).sum()\n",
        "                aw_loto_3_sur_5_et_C = ((df_results['bons_p'] == 3) & (df_results['bon_c'] == 1)).sum()\n",
        "                aw_loto_2_sur_5_et_C = ((df_results['bons_p'] == 2) & (df_results['bon_c'] == 1)).sum()\n",
        "                print(f\"    4 sur 5 N°P: {aw_loto_4_sur_5} fois\")\n",
        "                print(f\"    3 sur 5 N°P + N°C: {aw_loto_3_sur_5_et_C} fois\")\n",
        "                print(f\"    2 sur 5 N°P + N°C: {aw_loto_2_sur_5_et_C} fois\")\n",
        "\n",
        "                print(f\"\\n  'Quasiment-Gagnants' Personnalisés (Détails par tirage - Moyenne sur backtest):\")\n",
        "                aw_cols_num = [col for col in df_results.columns if col.startswith('aw_num_')]\n",
        "                aw_cols_comp = [col for col in df_results.columns if col.startswith('aw_comp_')]\n",
        "\n",
        "                print(\"    Pour les Numéros Principaux:\")\n",
        "                for col in aw_cols_num:\n",
        "                    col_name_fr = col.replace('aw_num_', '').replace('_', ' ').capitalize()\n",
        "                    print(f\"      {col_name_fr}: Moy. {df_results[col].mean():.2f}, Max {df_results[col].max()}, Total {df_results[col].sum()}\")\n",
        "\n",
        "                print(\"    Pour le Numéro Complémentaire:\")\n",
        "                for col in aw_cols_comp:\n",
        "                    col_name_fr = col.replace('aw_comp_', '').replace('_', ' ').capitalize()\n",
        "                    print(f\"      {col_name_fr}: Moy. {df_results[col].mean():.2f}, Max {df_results[col].max()}, Total {df_results[col].sum()}\")\n",
        "    print(\"=\"*60)\n",
        "    return df_backtest_custom, df_backtest_ml # Retourner les dataframes pour une analyse potentielle plus poussée\n",
        "\n",
        "\n",
        "def predire_prochain_tirage_final(lotto_data_complet_desc, poids_optimises):\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n \\\"PRÉDICTION\\\" FINALE POUR LE PROCHAIN TIRAGE (HYPOTHÉTIQUE) \\n\" + \"=\"*40)\n",
        "    print(\"ATTENTION : Ceci est un exercice théorique basé sur des modèles statistiques et de ML. NE PAS UTILISER POUR DES PARIS RÉELS.\")\n",
        "\n",
        "    historique_complet_asc = lotto_data_complet_desc.sort_values(by='Date', ascending=True).reset_index(drop=True)\n",
        "    if historique_complet_asc.empty: print(\"Aucune donnée historique pour la prédiction finale.\"); return\n",
        "\n",
        "    print(f\"Utilisation de {len(historique_complet_asc)} tirages historiques pour la prédiction finale.\")\n",
        "\n",
        "    stats_sur_historique_complet_custom = calculer_statistiques_avancees(historique_complet_asc.copy())\n",
        "    nombre_total_tirages_historiques = len(historique_complet_asc)\n",
        "    print(f\"\\n--- Prédiction avec Modèle Custom (utilisant les poids optimisés) ---\")\n",
        "    scores_p_custom_final = [{'numero':n, 'score':calculer_score_prediction_custom(n, stats_sur_historique_complet_custom, nombre_total_tirages_historiques, poids_optimises)} for n in range(1,50)]\n",
        "    scores_c_custom_final = [{'numero':n, 'score':calculer_score_prediction_custom(f\"C{n}\", stats_sur_historique_complet_custom, nombre_total_tirages_historiques, poids_optimises)} for n in range(1,11)]\n",
        "    scores_p_custom_final.sort(key=lambda x:x['score'], reverse=True)\n",
        "    scores_c_custom_final.sort(key=lambda x:x['score'], reverse=True)\n",
        "    pred_p_custom_final = sorted([item['numero'] for item in scores_p_custom_final[:5]])\n",
        "    pred_c_custom_final = scores_c_custom_final[0]['numero'] if scores_c_custom_final and scores_c_custom_final[0]['score'] > 0 else None\n",
        "    print(f\"  Numéros principaux \\\"prédits\\\" (Custom): {pred_p_custom_final}\")\n",
        "    print(f\"  Numéro complémentaire \\\"prédit\\\" (Custom): {pred_c_custom_final}\")\n",
        "    print(\"  Top 10 scores pour les numéros principaux (Custom):\")\n",
        "    for s in scores_p_custom_final[:10]: print(f\"    N°{s['numero']}: score {s['score']:.3f}\")\n",
        "\n",
        "\n",
        "    if RUN_BACKTESTING_ML_IN_FULL_ANALYSIS:\n",
        "        taille_fenetre_entrainement_final_ml = 105\n",
        "        debut_fenetre_final = max(0, len(historique_complet_asc) - taille_fenetre_entrainement_final_ml)\n",
        "        donnees_pour_entrainement_ml_final = historique_complet_asc.iloc[debut_fenetre_final:]\n",
        "        min_hist_pour_features_ml_final = 21\n",
        "\n",
        "        if len(donnees_pour_entrainement_ml_final) < min_hist_pour_features_ml_final:\n",
        "            print(f\"\\nFenêtre d'entraînement ML final trop petite ({len(donnees_pour_entrainement_ml_final)}). Prédictions ML par défaut.\")\n",
        "            pred_p_ml_final = sorted(list(MES_NUMEROS_PRINCIPAUX_JOUES))[:5] if MES_NUMEROS_PRINCIPAUX_JOUES else random.sample(range(1,50),5)\n",
        "            pred_c_ml_final = list(MES_COMPLEMENTAIRES_POSSIBLES_JOUES)[0] if MES_COMPLEMENTAIRES_POSSIBLES_JOUES else random.randint(1,10)\n",
        "            predictions_probas_p_ml_final_display = []\n",
        "        else:\n",
        "            print(f\"\\n--- Prédiction avec Modèle ML (Random Forest) ---\")\n",
        "            print(f\"  Entraînement des modèles ML finaux sur {len(donnees_pour_entrainement_ml_final)} tirages (indices {debut_fenetre_final} à {len(historique_complet_asc)-1}).\")\n",
        "\n",
        "            stats_pour_ml_final_prediction = calculer_statistiques_avancees(historique_complet_asc.copy())\n",
        "\n",
        "            modeles_ml_p_final, modeles_ml_c_final = {}, {}\n",
        "            predictions_probas_p_ml_final, predictions_probas_c_ml_final = [], []\n",
        "            param_grid_rf_final = {'n_estimators': [50, 100], 'max_depth': [None, 10], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 3]}\n",
        "            default_n_estimators_final = 100\n",
        "\n",
        "            for num_p_ml in range(1, 50):\n",
        "                X_train_ml, y_train_ml = preparer_donnees_pour_ml_numero(donnees_pour_entrainement_ml_final.copy(), num_p_ml, 'principal')\n",
        "                if len(X_train_ml) < 10 or len(np.unique(y_train_ml)) < 2:\n",
        "                    modeles_ml_p_final[num_p_ml] = None; predictions_probas_p_ml_final.append({'numero': num_p_ml, 'proba': 0.0}); continue\n",
        "\n",
        "                model_ml_p_trained = None\n",
        "                if USE_GRID_SEARCH_FOR_FULL_ANALYSIS_ML:\n",
        "                    cv_splits_pf = min(2, len(X_train_ml) // (sum(param_grid_rf_final.get('min_samples_split', [2])) // len(param_grid_rf_final.get('min_samples_split', [2]))) if len(X_train_ml) > 10 else 2)\n",
        "                    cv_splits_pf = max(2, cv_splits_pf)\n",
        "                    grid_search_p_final = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), param_grid_rf_final, cv=cv_splits_pf, scoring='roc_auc', n_jobs=-1, error_score='raise')\n",
        "                    try: grid_search_p_final.fit(X_train_ml, y_train_ml); model_ml_p_trained = grid_search_p_final.best_estimator_\n",
        "                    except ValueError as e_gs_pf:\n",
        "                        logging.warning(f\"Erreur GridSearchCV Principal Final N°{num_p_ml}: {e_gs_pf}. Modèle par défaut.\")\n",
        "                        model_ml_p_trained = RandomForestClassifier(n_estimators=default_n_estimators_final, random_state=42, class_weight='balanced', n_jobs=-1).fit(X_train_ml, y_train_ml)\n",
        "                else: model_ml_p_trained = RandomForestClassifier(n_estimators=default_n_estimators_final, random_state=42, class_weight='balanced', n_jobs=-1).fit(X_train_ml, y_train_ml)\n",
        "                modeles_ml_p_final[num_p_ml] = model_ml_p_trained\n",
        "\n",
        "                if num_p_ml not in stats_pour_ml_final_prediction:\n",
        "                    logging.warning(f\"Stats non trouvées N°P {num_p_ml} création features prédiction finale.\"); predictions_probas_p_ml_final.append({'numero': num_p_ml, 'proba': 0.0}); continue\n",
        "                stat_num_cible_ml_p = stats_pour_ml_final_prediction[num_p_ml]\n",
        "                s_t_p_pred_fin, a_3_d_pred_fin, a_5_d_pred_fin = _calculer_features_recence_pour_prediction(historique_complet_asc, num_p_ml, 'principal')\n",
        "                ecart_moyen_spec_feature_pred_pf = stat_num_cible_ml_p.get('ecart_moyen_specifique', float('inf'))\n",
        "                if ecart_moyen_spec_feature_pred_pf == float('inf'): ecart_moyen_spec_feature_pred_pf = len(historique_complet_asc) if stat_num_cible_ml_p.get('frequence_absolue',0) <=1 else -1\n",
        "                features_pour_prediction_ml_p = np.array([[\n",
        "                    stat_num_cible_ml_p.get('frequence_relative', 0.0), stat_num_cible_ml_p.get('ecart_actuel', len(historique_complet_asc)),\n",
        "                    ecart_moyen_spec_feature_pred_pf, len(stat_num_cible_ml_p.get('ecarts_observes', [])),\n",
        "                    s_t_p_pred_fin, a_3_d_pred_fin, a_5_d_pred_fin\n",
        "                ]])\n",
        "                if modeles_ml_p_final[num_p_ml]:\n",
        "                    try: proba_p_ml = modeles_ml_p_final[num_p_ml].predict_proba(features_pour_prediction_ml_p)[0][1]; predictions_probas_p_ml_final.append({'numero': num_p_ml, 'proba': proba_p_ml})\n",
        "                    except Exception as e_pred_pf: logging.error(f\"Erreur predict_proba Principal Final N°{num_p_ml}: {e_pred_pf}\"); predictions_probas_p_ml_final.append({'numero': num_p_ml, 'proba': 0.0})\n",
        "                else: predictions_probas_p_ml_final.append({'numero': num_p_ml, 'proba': 0.0})\n",
        "\n",
        "            for num_c_ml in range(1, 11):\n",
        "                X_train_ml_c, y_train_ml_c = preparer_donnees_pour_ml_numero(donnees_pour_entrainement_ml_final.copy(), num_c_ml, 'complementaire')\n",
        "                cle_stat_ml_c = f\"C{num_c_ml}\"\n",
        "                if len(X_train_ml_c) < 10 or len(np.unique(y_train_ml_c)) < 2:\n",
        "                    modeles_ml_c_final[num_c_ml] = None; predictions_probas_c_ml_final.append({'numero': num_c_ml, 'proba': 0.0}); continue\n",
        "\n",
        "                model_ml_c_trained = None\n",
        "                if USE_GRID_SEARCH_FOR_FULL_ANALYSIS_ML:\n",
        "                    cv_splits_cf = min(2, len(X_train_ml_c) // (sum(param_grid_rf_final.get('min_samples_split', [2])) // len(param_grid_rf_final.get('min_samples_split', [2]))) if len(X_train_ml_c) > 10 else 2)\n",
        "                    cv_splits_cf = max(2, cv_splits_cf)\n",
        "                    grid_search_c_final = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), param_grid_rf_final, cv=cv_splits_cf, scoring='roc_auc', n_jobs=-1, error_score='raise')\n",
        "                    try: grid_search_c_final.fit(X_train_ml_c, y_train_ml_c); model_ml_c_trained = grid_search_c_final.best_estimator_\n",
        "                    except ValueError as e_gs_cf:\n",
        "                        logging.warning(f\"Erreur GridSearchCV Complémentaire Final N°{num_c_ml}: {e_gs_cf}. Modèle par défaut.\")\n",
        "                        model_ml_c_trained = RandomForestClassifier(n_estimators=default_n_estimators_final, random_state=42, class_weight='balanced', n_jobs=-1).fit(X_train_ml_c, y_train_ml_c)\n",
        "                else: model_ml_c_trained = RandomForestClassifier(n_estimators=default_n_estimators_final, random_state=42, class_weight='balanced', n_jobs=-1).fit(X_train_ml_c, y_train_ml_c)\n",
        "                modeles_ml_c_final[num_c_ml] = model_ml_c_trained\n",
        "\n",
        "                if cle_stat_ml_c not in stats_pour_ml_final_prediction:\n",
        "                    logging.warning(f\"Stats non trouvées N°C {num_c_ml} création features prédiction finale.\"); predictions_probas_c_ml_final.append({'numero': num_c_ml, 'proba': 0.0}); continue\n",
        "                stat_num_cible_ml_c = stats_pour_ml_final_prediction[cle_stat_ml_c]\n",
        "                s_t_p_pred_fin_c, a_3_d_pred_fin_c, a_5_d_pred_fin_c = _calculer_features_recence_pour_prediction(historique_complet_asc, num_c_ml, 'complementaire')\n",
        "                ecart_moyen_spec_feature_pred_cf = stat_num_cible_ml_c.get('ecart_moyen_specifique', float('inf'))\n",
        "                if ecart_moyen_spec_feature_pred_cf == float('inf'): ecart_moyen_spec_feature_pred_cf = len(historique_complet_asc) if stat_num_cible_ml_c.get('frequence_absolue',0) <=1 else -1\n",
        "                features_pour_prediction_ml_c = np.array([[\n",
        "                    stat_num_cible_ml_c.get('frequence_relative', 0.0), stat_num_cible_ml_c.get('ecart_actuel', len(historique_complet_asc)),\n",
        "                    ecart_moyen_spec_feature_pred_cf, len(stat_num_cible_ml_c.get('ecarts_observes', [])),\n",
        "                    s_t_p_pred_fin_c, a_3_d_pred_fin_c, a_5_d_pred_fin_c\n",
        "                ]])\n",
        "                if modeles_ml_c_final[num_c_ml]:\n",
        "                    try: proba_c_ml = modeles_ml_c_final[num_c_ml].predict_proba(features_pour_prediction_ml_c)[0][1]; predictions_probas_c_ml_final.append({'numero': num_c_ml, 'proba': proba_c_ml})\n",
        "                    except Exception as e_pred_cf: logging.error(f\"Erreur predict_proba Complémentaire Final N°{num_c_ml}: {e_pred_cf}\"); predictions_probas_c_ml_final.append({'numero': num_c_ml, 'proba': 0.0})\n",
        "                else: predictions_probas_c_ml_final.append({'numero': num_c_ml, 'proba': 0.0})\n",
        "\n",
        "            predictions_probas_p_ml_final.sort(key=lambda x: x['proba'], reverse=True); predictions_probas_c_ml_final.sort(key=lambda x: x['proba'], reverse=True)\n",
        "            pred_p_ml_final = sorted([item['numero'] for item in predictions_probas_p_ml_final[:5]])\n",
        "            pred_c_ml_final = predictions_probas_c_ml_final[0]['numero'] if predictions_probas_c_ml_final and predictions_probas_c_ml_final[0]['proba'] > 0 else None\n",
        "            predictions_probas_p_ml_final_display = predictions_probas_p_ml_final\n",
        "\n",
        "        print(f\"  Numéros principaux \\\"prédits\\\" (ML): {pred_p_ml_final}\")\n",
        "        print(f\"  Numéro complémentaire \\\"prédit\\\" (ML): {pred_c_ml_final}\")\n",
        "        if predictions_probas_p_ml_final_display :\n",
        "            print(\"  Top 10 probabilités pour les numéros principaux (ML):\")\n",
        "            for s in predictions_probas_p_ml_final_display[:10]: print(f\"    N°{s['numero']}: proba {s['proba']:.3f}\")\n",
        "    else:\n",
        "        print(\"\\nPrédiction finale ML désactivée (RUN_BACKTESTING_ML_IN_FULL_ANALYSIS=False).\")\n",
        "\n",
        "    print(\"\\nRAPPEL IMPORTANT : Les résultats de ce script sont purement théoriques et issus d'un exercice de modélisation.\")\n",
        "    print(\"Le Loto est un jeu de hasard. Ne basez aucune décision financière sur ces \\\"prédictions\\\".\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def analyser_performance_mes_numeros(lotto_data, numeros_principaux_joues, complementaires_joues_possibles):\n",
        "    print(f\"\\n\" + \"=\"*40); print(f\" ANALYSE DE PERFORMANCE POUR VOS NUMÉROS JOUÉS \"); print(f\" Numéros principaux joués: {numeros_principaux_joues}\"); print(f\" Complémentaires potentiels: {complementaires_joues_possibles}\"); print(f\"=\"*40)\n",
        "    if lotto_data.empty:\n",
        "        print(\"Aucune donnée pour analyse de vos numéros.\");\n",
        "        return None\n",
        "\n",
        "    num_cols = [f\"N{i}\" for i in range(1, 6)]\n",
        "    if not all(c in lotto_data.columns for c in num_cols) or 'Complementaire' not in lotto_data.columns:\n",
        "        print(\"Colonnes manquantes.\");\n",
        "        return None\n",
        "\n",
        "    # La variable 'nombre_total_tirages' est définie ici et sera utilisée plus bas.\n",
        "    nombre_total_tirages = len(lotto_data)\n",
        "    gains = {\"5P\":0,\"5P+C\":0,\"4P\":0,\"4P+C\":0,\"3P\":0,\"3P+C\":0,\"2P\":0,\"2P+C\":0,\"1P+C\":0,\"0P+C\":0}\n",
        "    combinaison_jouee_gagnante_count, comp_6_gagnant, comp_7_gagnant, comp_9_gagnant = 0,0,0,0\n",
        "\n",
        "    for _, tirage_row in lotto_data.iterrows():\n",
        "        try:\n",
        "            gagnants_p_set = {int(tirage_row[n]) for n in num_cols if pd.notna(tirage_row[n])}\n",
        "            if len(gagnants_p_set) != 5 : logging.warning(f\"Tirage incomplet: {tirage_row.get('Date','N/A')}\"); continue\n",
        "            comp_gagnant = int(tirage_row['Complementaire']) if pd.notna(tirage_row['Complementaire']) else None\n",
        "            if comp_gagnant is None: logging.warning(f\"Complémentaire manquant: {tirage_row.get('Date','N/A')}\"); continue\n",
        "        except ValueError: logging.error(f\"Erreur conversion: {tirage_row.get('Date','N/A')}\"); continue\n",
        "\n",
        "        corrects_p = len(numeros_principaux_joues.intersection(gagnants_p_set))\n",
        "        correct_c = comp_gagnant in complementaires_joues_possibles\n",
        "\n",
        "        if correct_c:\n",
        "            if comp_gagnant == 6: comp_6_gagnant +=1\n",
        "            if comp_gagnant == 7: comp_7_gagnant +=1\n",
        "            if comp_gagnant == 9: comp_9_gagnant +=1\n",
        "\n",
        "        if corrects_p == 5 and correct_c: gains[\"5P+C\"] += 1\n",
        "        elif corrects_p == 5: gains[\"5P\"] += 1\n",
        "        elif corrects_p == 4 and correct_c: gains[\"4P+C\"] += 1\n",
        "        elif corrects_p == 4: gains[\"4P\"] += 1\n",
        "        elif corrects_p == 3 and correct_c: gains[\"3P+C\"] += 1\n",
        "        elif corrects_p == 3: gains[\"3P\"] += 1\n",
        "        elif corrects_p == 2 and correct_c: gains[\"2P+C\"] += 1\n",
        "        elif corrects_p == 2: gains[\"2P\"] += 1\n",
        "        elif corrects_p == 1 and correct_c: gains[\"1P+C\"] += 1\n",
        "        elif corrects_p == 0 and correct_c: gains[\"0P+C\"] += 1\n",
        "\n",
        "        if numeros_principaux_joues.issubset(gagnants_p_set): combinaison_jouee_gagnante_count +=1\n",
        "\n",
        "    print(f\"Sur {nombre_total_tirages} tirages analysés...\")\n",
        "    for rang, count in gains.items():\n",
        "        if count > 0: print(f\"  Rang '{rang}': {count} fois ({(count/nombre_total_tirages)*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nCombinaison {numeros_principaux_joues} sortie en entier: {combinaison_jouee_gagnante_count} fois.\")\n",
        "    print(f\"Complémentaire 6 (un de vos possibles) sorti: {comp_6_gagnant} fois.\")\n",
        "    print(f\"Complémentaire 7 (un de vos possibles) sorti: {comp_7_gagnant} fois.\")\n",
        "    print(f\"Complémentaire 9 (un de vos possibles) sorti: {comp_9_gagnant} fois.\")\n",
        "\n",
        "    if nombre_total_tirages > 0:\n",
        "        k_joues = len(numeros_principaux_joues)\n",
        "        k_corrects_cible = 3\n",
        "        if k_joues >= k_corrects_cible:\n",
        "            prob_kP_theorique = (nCr(k_joues, k_corrects_cible) * nCr(49-k_joues, 5-k_corrects_cible)) / nCr(49,5)\n",
        "            attendus_kP = nombre_total_tirages * prob_kP_theorique\n",
        "            print(f\"\\nPour '{k_corrects_cible}P' (vos {k_corrects_cible} principaux corrects parmi les 5 tirés, avec {k_joues} joués) :\")\n",
        "            print(f\"  Observé : {gains.get(f'{k_corrects_cible}P',0)} fois\")\n",
        "            print(f\"  Attendu (théoriquement) : {attendus_kP:.2f} fois (Proba: {prob_kP_theorique:.6f})\")\n",
        "    return gains\n",
        "\n",
        "# ---- Exécution Principale ----\n",
        "if __name__ == \"__main__\":\n",
        "    start_time_script = time.time()\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "    print(\"Début du script Loto...\")\n",
        "\n",
        "    if RUN_LAMBDA_STRATEGY_MODE:\n",
        "        print(\"\\n\" + \"*\"*10 + \" MODE STRATÉGIE LAMBDA ACTIVÉ \" + \"*\"*10)\n",
        "        strategie_loto_lambda_main()\n",
        "    else:\n",
        "        print(\"\\n\" + \"*\"*10 + \" MODE ANALYSE COMPLÈTE ACTIVÉ \" + \"*\"*10)\n",
        "        lotto_data_desc_full = fetch_lotto_data()\n",
        "\n",
        "        if lotto_data_desc_full is not None and not lotto_data_desc_full.empty:\n",
        "            print(f\"\\nDonnées totales récupérées initialement : {len(lotto_data_desc_full)} tirages.\")\n",
        "\n",
        "            print(\"Filtrage pour ne conserver que les tirages du Samedi (tous les mois)...\")\n",
        "            if 'Jour' in lotto_data_desc_full.columns and 'Date' in lotto_data_desc_full.columns:\n",
        "                donnees_finales_filtrees_desc = lotto_data_desc_full[\n",
        "                    lotto_data_desc_full['Jour'].astype(str).str.contains('Samedi', case=False, na=False)\n",
        "                ].copy()\n",
        "                if not donnees_finales_filtrees_desc.empty:\n",
        "                    donnees_finales_filtrees_desc['Date'] = pd.to_datetime(donnees_finales_filtrees_desc['Date'], errors='coerce')\n",
        "                    donnees_finales_filtrees_desc.dropna(subset=['Date'], inplace=True)\n",
        "                print(f\"Nombre de tirages du Samedi (tous mois) conservés : {len(donnees_finales_filtrees_desc)}.\")\n",
        "            else:\n",
        "                print(\"Colonnes 'Jour' ou 'Date' manquantes. Utilisation des données non filtrées.\")\n",
        "                donnees_finales_filtrees_desc = lotto_data_desc_full.copy()\n",
        "\n",
        "\n",
        "            if donnees_finales_filtrees_desc.empty:\n",
        "                print(\"Aucun tirage après filtrage (ou aucune donnée initiale). Arrêt.\")\n",
        "            else:\n",
        "                print(f\"Nombre de tirages à analyser : {len(donnees_finales_filtrees_desc)}.\")\n",
        "                lotto_data_asc_filtrees = donnees_finales_filtrees_desc.sort_values(by='Date', ascending=True).reset_index(drop=True)\n",
        "\n",
        "                if RUN_SLIDING_WINDOW_ENHANCED_METRICS:\n",
        "                    analyze_sliding_windows(lotto_data_asc_filtrees.copy(), window_size=105, step_size=28)\n",
        "\n",
        "                analysis_results = run_full_statistical_analysis(lotto_data_asc_filtrees.copy())\n",
        "                analyser_performance_mes_numeros(lotto_data_asc_filtrees.copy(), MES_NUMEROS_PRINCIPAUX_JOUES, MES_COMPLEMENTAIRES_POSSIBLES_JOUES)\n",
        "\n",
        "                if 'gaps' in analysis_results and analysis_results.get('gaps'):\n",
        "                    gaps_data_result = analysis_results['gaps']\n",
        "                    print(f\"\\n--- Analyse des Écarts pour VOS numéros joués ({MES_NUMEROS_PRINCIPAUX_JOUES}) ---\")\n",
        "                    for num_perso in sorted(list(MES_NUMEROS_PRINCIPAUX_JOUES)):\n",
        "                        if num_perso in gaps_data_result and gaps_data_result.get(num_perso):\n",
        "                            print(f\"  Pour le numéro {num_perso}:\")\n",
        "                            print(f\"    Écarts observés (partiel): {gaps_data_result[num_perso][:10]}...\")\n",
        "                            print(f\"    Moyenne des écarts: {np.mean(gaps_data_result[num_perso]):.2f}\")\n",
        "                            print(f\"    Écart max observé: {np.max(gaps_data_result[num_perso])}\")\n",
        "                            if num_perso in analysis_results.get('frequencies', {}):\n",
        "                                derniere_apparition_index = -1\n",
        "                                for idx, row_draw in reversed(list(lotto_data_asc_filtrees.iterrows())):\n",
        "                                    try:\n",
        "                                        numeros_du_tirage = {int(row_draw[f'N{k}']) for k in range(1,6) if pd.notna(row_draw[f'N{k}'])}\n",
        "                                        if num_perso in numeros_du_tirage:\n",
        "                                            derniere_apparition_index = idx; break\n",
        "                                    except ValueError: logging.warning(f\"Skipping row in gap (perso) due to int conversion: {row_draw}\"); continue\n",
        "                                if derniere_apparition_index != -1:\n",
        "                                    ecart_actuel_perso = len(lotto_data_asc_filtrees) - 1 - derniere_apparition_index\n",
        "                                    print(f\"    Écart actuel (dataset filtré): {ecart_actuel_perso}\")\n",
        "                                else: print(f\"    Écart actuel (dataset filtré): Jamais vu ou dernier tirage.\")\n",
        "                        else: print(f\"  Pas de données d'écarts pour N°{num_perso} dans ce dataset.\")\n",
        "\n",
        "\n",
        "                min_hist_opti = 63\n",
        "                min_tirages_backtest_opti = max(49, int(len(lotto_data_asc_filtrees) * 0.1))\n",
        "                min_tirages_backtest_opti = min(min_tirages_backtest_opti, 105)\n",
        "\n",
        "                if len(lotto_data_asc_filtrees) > min_hist_opti + min_tirages_backtest_opti:\n",
        "                    poids_optimises_deterministe = optimiser_poids_par_grille(\n",
        "                        lotto_data_asc_filtrees.copy(),\n",
        "                        nombre_tirages_backtest=min_tirages_backtest_opti,\n",
        "                        min_hist_req=min_hist_opti\n",
        "                    )\n",
        "                else:\n",
        "                    print(f\"\\nPas assez de données ({len(lotto_data_asc_filtrees)}) pour l'optimisation des poids. Poids par défaut.\")\n",
        "                    poids_optimises_deterministe = {'frequence':0.1, 'proximite_ecart_moyen_spec':0.5, 'depassement_ecart_moyen_spec':0.3, 'proximite_ecart_global_ref':0.1, 'depassement_ecart_global_ref':0.05, 'bonus_jamais_sorti':0.1, 'bonus_rare':0.05}\n",
        "\n",
        "                if RUN_TRANSITION_PROBABILITIES:\n",
        "                    analyze_transitions(lotto_data_asc_filtrees.copy(), lag=1, top_n=15)\n",
        "\n",
        "                if RUN_DRAW_CLUSTERING:\n",
        "                    df_clustered = cluster_draws(lotto_data_asc_filtrees.copy(), CONFIG_LOTO_LAMBDA, n_clusters_kmeans=6)\n",
        "                    if df_clustered is not None and 'cluster_kmeans' in df_clustered.columns:\n",
        "                        print(\"\\nAperçu des données avec clusters K-Means (si réussi):\")\n",
        "                        print(df_clustered[['Date', 'N1', 'N2', 'N3', 'N4', 'N5', 'cluster_kmeans']].tail().to_string())\n",
        "\n",
        "                if RUN_PREFIXSPAN_ANALYSIS:\n",
        "                    analyze_frequent_sequences_prefixspan(lotto_data_asc_filtrees.copy(), min_support_ratio=0.005, max_pattern_length=5)\n",
        "\n",
        "\n",
        "                if RUN_BACKTESTING_ML_IN_FULL_ANALYSIS:\n",
        "                    min_backtest_hist_global = 63\n",
        "                    nb_backtest_tirages_a_tester = max(11, int(len(lotto_data_asc_filtrees) * 0.02))\n",
        "                    nb_backtest_tirages_a_tester = min(nb_backtest_tirages_a_tester, 28) # Limiter le nombre de tirages pour le backtest\n",
        "                    taille_fenetre_ml = 105\n",
        "\n",
        "                    needed_for_ml_backtest = taille_fenetre_ml + nb_backtest_tirages_a_tester\n",
        "                    needed_for_custom_backtest = min_backtest_hist_global + nb_backtest_tirages_a_tester\n",
        "\n",
        "                    if len(lotto_data_asc_filtrees) >= max(needed_for_ml_backtest, needed_for_custom_backtest) :\n",
        "                        # La fonction retourne maintenant les DataFrames pour une analyse plus poussée si besoin\n",
        "                        df_results_custom, df_results_ml = tester_performance_predictive_historique(\n",
        "                            lotto_data_asc_filtrees.copy(),\n",
        "                            nombre_de_tirages_a_tester=nb_backtest_tirages_a_tester,\n",
        "                            poids_optimises=poids_optimises_deterministe\n",
        "                        )\n",
        "                        # Vous pouvez maintenant faire quelque chose avec df_results_custom et df_results_ml\n",
        "                        # Par exemple, sauvegarder en CSV ou afficher plus de stats\n",
        "                        if df_results_custom is not None and not df_results_custom.empty: # Vérification ajoutée\n",
        "                             print(\"\\nDataFrame des résultats détaillés du backtest (Modèle Custom):\")\n",
        "                             print(df_results_custom.head().to_string())\n",
        "                        if df_results_ml is not None and not df_results_ml.empty: # Vérification ajoutée\n",
        "                             print(\"\\nDataFrame des résultats détaillés du backtest (Modèle ML):\")\n",
        "                             print(df_results_ml.head().to_string())\n",
        "\n",
        "                    else: print(f\"\\nPas assez de données pour backtesting comparatif.\")\n",
        "                else: print(\"\\nBacktesting ML désactivé (RUN_BACKTESTING_ML_IN_FULL_ANALYSIS=False).\")\n",
        "\n",
        "                if len(lotto_data_asc_filtrees) > min_hist_opti : # Utiliser min_hist_opti qui est la même que pour l'optimisation\n",
        "                    predire_prochain_tirage_final(donnees_finales_filtrees_desc.copy(), poids_optimises_deterministe)\n",
        "                else: print(f\"\\nPas assez de données pour la prédiction finale.\")\n",
        "        else:\n",
        "            print(\"Aucune donnée de loterie n'a pu être chargée initialement. Arrêt du script.\")\n",
        "\n",
        "    end_time_script = time.time()\n",
        "    print(f\"\\nScript terminé. Durée totale d'exécution : {time.strftime('%H:%M:%S', time.gmtime(end_time_script - start_time_script))}.\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c3faf917f698>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprefixspan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrefixSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpmdarima\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m \u001b[0;31m# Ajout de l'import pour auto_arima\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Configuration logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Stuff we want at top-level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_arima\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARIMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoARIMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStepwiseContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0macf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocorr_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_acf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_pacf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtsdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/arima/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapprox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/arima/approx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_endog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pmdarima/utils/array.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_array\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mC_intgrt_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m __all__ = [\n",
            "\u001b[0;32mpmdarima/utils/_array.pyx\u001b[0m in \u001b[0;36minit pmdarima.utils._array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pJO96MOjlwnl",
        "outputId": "07dc5a50-a167-454b-fe5e-bec14bcb79cc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}